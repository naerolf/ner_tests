{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ac57fd-8ec4-4f27-91dc-fedf6714d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable jedi autocompleter\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80225757-b0f7-4f3f-8de9-1df3c9610a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements_custom.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements_custom.txt\n",
    "transformers==4.6.1\n",
    "torch==1.8.1\n",
    "torchvision==0.9.1\n",
    "pandas==1.1.5\n",
    "sklearn==0.0\n",
    "matplotlib==3.4.2\n",
    "ipywidgets==7.6.3\n",
    "datasets==1.6.2\n",
    "seqeval==1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b28edb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.6.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from -r requirements_custom.txt (line 1)) (4.6.1)\n",
      "Requirement already satisfied: torch==1.8.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from -r requirements_custom.txt (line 2)) (1.8.1)\n",
      "Requirement already satisfied: torchvision==0.9.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from -r requirements_custom.txt (line 3)) (0.9.1)\n",
      "Requirement already satisfied: pandas==1.1.5 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from -r requirements_custom.txt (line 4)) (1.1.5)\n",
      "Requirement already satisfied: sklearn==0.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from -r requirements_custom.txt (line 5)) (0.0)\n",
      "Requirement already satisfied: matplotlib==3.4.2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from -r requirements_custom.txt (line 6)) (3.4.2)\n",
      "Requirement already satisfied: ipywidgets==7.6.3 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from -r requirements_custom.txt (line 7)) (7.6.3)\n",
      "Requirement already satisfied: datasets==1.6.2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from -r requirements_custom.txt (line 8)) (1.6.2)\n",
      "Requirement already satisfied: seqeval==1.2.2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from -r requirements_custom.txt (line 9)) (1.2.2)\n",
      "Requirement already satisfied: packaging in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (20.9)\n",
      "Requirement already satisfied: requests in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (2.25.1)\n",
      "Requirement already satisfied: sacremoses in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (0.0.45)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (4.49.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (1.20.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (2021.4.4)\n",
      "Requirement already satisfied: filelock in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (0.0.8)\n",
      "Requirement already satisfied: typing-extensions in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from torch==1.8.1->-r requirements_custom.txt (line 2)) (3.10.0.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from torchvision==0.9.1->-r requirements_custom.txt (line 3)) (8.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from pandas==1.1.5->-r requirements_custom.txt (line 4)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from pandas==1.1.5->-r requirements_custom.txt (line 4)) (2021.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from sklearn==0.0->-r requirements_custom.txt (line 5)) (0.24.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from matplotlib==3.4.2->-r requirements_custom.txt (line 6)) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from matplotlib==3.4.2->-r requirements_custom.txt (line 6)) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from matplotlib==3.4.2->-r requirements_custom.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (7.22.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (3.5.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (5.1.3)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (5.3.4)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (5.0.5)\n",
      "Requirement already satisfied: dill in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from datasets==1.6.2->-r requirements_custom.txt (line 8)) (0.3.3)\n",
      "Requirement already satisfied: fsspec in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from datasets==1.6.2->-r requirements_custom.txt (line 8)) (2021.5.0)\n",
      "Requirement already satisfied: xxhash in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from datasets==1.6.2->-r requirements_custom.txt (line 8)) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from datasets==1.6.2->-r requirements_custom.txt (line 8)) (0.70.11.1)\n",
      "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from datasets==1.6.2->-r requirements_custom.txt (line 8)) (4.0.0)\n",
      "Requirement already satisfied: six in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from cycler>=0.10->matplotlib==3.4.2->-r requirements_custom.txt (line 6)) (1.15.0)\n",
      "Requirement already satisfied: appnope in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.1.2)\n",
      "Requirement already satisfied: jupyter-client in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (6.1.12)\n",
      "Requirement already satisfied: tornado>=4.2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (6.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (4.8.0)\n",
      "Requirement already satisfied: decorator in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (5.0.9)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.17.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (3.0.17)\n",
      "Requirement already satisfied: pygments in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (2.9.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (52.0.0.post20210125)\n",
      "Requirement already satisfied: backcall in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.7.5)\n",
      "Requirement already satisfied: parso>=0.7.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.8.2)\n",
      "Requirement already satisfied: ipython-genutils in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (3.2.0)\n",
      "Requirement already satisfied: jupyter-core in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (4.7.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.17.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.2.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from requests->transformers==4.6.1->-r requirements_custom.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from requests->transformers==4.6.1->-r requirements_custom.txt (line 1)) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from requests->transformers==4.6.1->-r requirements_custom.txt (line 1)) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from requests->transformers==4.6.1->-r requirements_custom.txt (line 1)) (2.10)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from scikit-learn->sklearn==0.0->-r requirements_custom.txt (line 5)) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from scikit-learn->sklearn==0.0->-r requirements_custom.txt (line 5)) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from scikit-learn->sklearn==0.0->-r requirements_custom.txt (line 5)) (1.6.3)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (6.4.0)\n",
      "Requirement already satisfied: nbconvert in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (6.0.7)\n",
      "Requirement already satisfied: argon2-cffi in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (20.1.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (20.0.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.9.4)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.5.0)\n",
      "Requirement already satisfied: prometheus-client in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.10.1)\n",
      "Requirement already satisfied: jinja2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (3.0.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0.0rc2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (2.0.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.4.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.8.4)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.5.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.1.2)\n",
      "Requirement already satisfied: bleach in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (3.3.0)\n",
      "Requirement already satisfied: defusedxml in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.7.1)\n",
      "Requirement already satisfied: testpath in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.4.4)\n",
      "Requirement already satisfied: nest-asyncio in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.5.1)\n",
      "Requirement already satisfied: async-generator in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.10)\n",
      "Requirement already satisfied: webencodings in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.5.1)\n",
      "Requirement already satisfied: click in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from sacremoses->transformers==4.6.1->-r requirements_custom.txt (line 1)) (8.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements_custom.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11796a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd7cc3a-b80b-4b75-a9f3-74bab9f77eb5",
   "metadata": {},
   "source": [
    "# NER BETO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42380dac-9cd7-4f19-b6bf-e6283bc891b2",
   "metadata": {},
   "source": [
    "A modo de ejemplo, usaremos la implementación mostrada en [Token Classification](https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb), ajustada al caso de BETO [publicación asociada](https://users.dcc.uchile.cl/~jperez/papers/pml4dc2020.pdf). Otro ejemplo interesante a revisar es [TinyBERT](https://huggingface.co/mrm8488/TinyBERT-spanish-uncased-finetuned-ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73cff9bb-c77d-4420-af57-ceb1afbaf0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from transformers import BertForMaskedLM, BertTokenizer\n",
    "# from transformers import BertForTokenClassification\n",
    "from transformers import DistilBertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8abce13b-9a1a-46a8-8734-5a1672a8a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"distilbert-base-cased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85b41da-cd00-4b8f-9def-f72182df487b",
   "metadata": {},
   "source": [
    "Definimos el dispositivo sobre el cual se hará entrenamiento, en caso de tener disponible una GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10e4d762-22a3-4562-8e59-fdbd64bca4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "376bb99c-3493-43d1-a3a6-71cb0a86cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertTokenizerFast.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bb6ba8-3b4c-409b-b84b-43fcd2b93389",
   "metadata": {},
   "source": [
    "## Carga de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59bc8824-0e19-47b1-9a3c-92e59172e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "767b7386-e97c-4226-8ec1-bd6be42c0d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/iacastro/PycharmProjects/ner_tests/data/raw/archive/ner_dataset.csv'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_folder = os.getcwd().split('ner_tests')[0]\n",
    "project_folder = '{}ner_tests'.format(root_folder)\n",
    "# project_folder\n",
    "data_path = '{}/data/raw/archive/ner_dataset.csv'.format(project_folder)\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0b2b6b5-ba52-4e1f-8718-cd50c894ce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner = pd.read_csv(data_path,delimiter=',',encoding='latin1',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1e5d117-bbea-41ba-9620-f72cda224e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner['Sentence #'] = df_ner['Sentence #'].ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff56b994-7c09-4760-b636-728181619333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demand</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>British</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>troops</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>from</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>that</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>country</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Families</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>soldiers</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>killed</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>conflict</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>joined</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>protesters</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>who</td>\n",
       "      <td>WP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>carried</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>banners</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>with</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>such</td>\n",
       "      <td>JJ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>slogans</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>as</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>\"</td>\n",
       "      <td>``</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Bush</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-per</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Number</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>One</td>\n",
       "      <td>CD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Terrorist</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>\"</td>\n",
       "      <td>``</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>\"</td>\n",
       "      <td>``</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Stop</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #           Word  POS    Tag\n",
       "0   Sentence: 1      Thousands  NNS      O\n",
       "1   Sentence: 1             of   IN      O\n",
       "2   Sentence: 1  demonstrators  NNS      O\n",
       "3   Sentence: 1           have  VBP      O\n",
       "4   Sentence: 1        marched  VBN      O\n",
       "5   Sentence: 1        through   IN      O\n",
       "6   Sentence: 1         London  NNP  B-geo\n",
       "7   Sentence: 1             to   TO      O\n",
       "8   Sentence: 1        protest   VB      O\n",
       "9   Sentence: 1            the   DT      O\n",
       "10  Sentence: 1            war   NN      O\n",
       "11  Sentence: 1             in   IN      O\n",
       "12  Sentence: 1           Iraq  NNP  B-geo\n",
       "13  Sentence: 1            and   CC      O\n",
       "14  Sentence: 1         demand   VB      O\n",
       "15  Sentence: 1            the   DT      O\n",
       "16  Sentence: 1     withdrawal   NN      O\n",
       "17  Sentence: 1             of   IN      O\n",
       "18  Sentence: 1        British   JJ  B-gpe\n",
       "19  Sentence: 1         troops  NNS      O\n",
       "20  Sentence: 1           from   IN      O\n",
       "21  Sentence: 1           that   DT      O\n",
       "22  Sentence: 1        country   NN      O\n",
       "23  Sentence: 1              .    .      O\n",
       "24  Sentence: 2       Families  NNS      O\n",
       "25  Sentence: 2             of   IN      O\n",
       "26  Sentence: 2       soldiers  NNS      O\n",
       "27  Sentence: 2         killed  VBN      O\n",
       "28  Sentence: 2             in   IN      O\n",
       "29  Sentence: 2            the   DT      O\n",
       "30  Sentence: 2       conflict   NN      O\n",
       "31  Sentence: 2         joined  VBD      O\n",
       "32  Sentence: 2            the   DT      O\n",
       "33  Sentence: 2     protesters  NNS      O\n",
       "34  Sentence: 2            who   WP      O\n",
       "35  Sentence: 2        carried  VBD      O\n",
       "36  Sentence: 2        banners  NNS      O\n",
       "37  Sentence: 2           with   IN      O\n",
       "38  Sentence: 2           such   JJ      O\n",
       "39  Sentence: 2        slogans  NNS      O\n",
       "40  Sentence: 2             as   IN      O\n",
       "41  Sentence: 2              \"   ``      O\n",
       "42  Sentence: 2           Bush  NNP  B-per\n",
       "43  Sentence: 2         Number   NN      O\n",
       "44  Sentence: 2            One   CD      O\n",
       "45  Sentence: 2      Terrorist   NN      O\n",
       "46  Sentence: 2              \"   ``      O\n",
       "47  Sentence: 2            and   CC      O\n",
       "48  Sentence: 2              \"   ``      O\n",
       "49  Sentence: 2           Stop   VB      O"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ner.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99f66ef6-ad5d-4278-a1dd-68a26824c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_texts = df_ner['Sentence #'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcd49330-dd3e-46e6-b80d-06831e622f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df_ner.groupby('Sentence #')['Word'].apply(list).to_list()#.groupby(level=0).apply(list)\n",
    "tags = df_ner.groupby('Sentence #')['Tag'].apply(list).to_list()#.groupby(level=0).apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d92c9160-7f01-48b4-93b9-277827c6ce82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['war', 'in', 'Iraq', 'and', 'demand', 'the', 'withdrawal']\n",
      "['O', 'O', 'B-geo', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(texts[0][10:17], tags[0][10:17], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1e723bb-f8bc-4064-b2a8-dcfbdd2671db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb58e8c-562f-4fbe-bbed-860b85c2f429",
   "metadata": {},
   "source": [
    "Generaremos encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ce43908-e862-40bd-bdbb-8c0137dd5570",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = set(tag for doc in tags for tag in doc)\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c624e354-60e9-4943-b194-8cff1eadb1ab",
   "metadata": {},
   "source": [
    "Se toman pads para tokenizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26c2d465-82ad-4da3-86db-041d1410f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_checkpoint)\n",
    "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb0c4e-eb64-47ec-ac6e-93d3431b9faa",
   "metadata": {},
   "source": [
    "Se codifican los tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0aa5d70-86a8-48ee-891f-2dd8379d3a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def encode_tags(tags, encodings):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    count = 0\n",
    "    error_encodings = []\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "        # print(count)\n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "        # print(arr_offset[:,0])\n",
    "        try:\n",
    "            doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "            encoded_labels.append(doc_enc_labels.tolist())\n",
    "        except:\n",
    "            error_encodings.append(count)\n",
    "        count += 1\n",
    "\n",
    "    return encoded_labels,error_encodings\n",
    "\n",
    "train_labels, train_errors = encode_tags(train_tags, train_encodings)\n",
    "val_labels, val_errors = encode_tags(val_tags, val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adfb45ae-5c47-4915-b1a0-311dab422234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[340,\n",
       " 4077,\n",
       " 12605,\n",
       " 14686,\n",
       " 15798,\n",
       " 16941,\n",
       " 22161,\n",
       " 22350,\n",
       " 23028,\n",
       " 24107,\n",
       " 25383,\n",
       " 28715,\n",
       " 28828,\n",
       " 31176,\n",
       " 31703,\n",
       " 33963,\n",
       " 34123,\n",
       " 35047]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9f58744-d977-4d15-ba4c-276c1b57375e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[699, 4864, 5388]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "351dae01-2314-4d40-873d-8aac96dbd38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
    "val_encodings.pop(\"offset_mapping\")\n",
    "train_dataset = NERDataset(train_encodings, train_labels)\n",
    "val_dataset = NERDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f133b-5807-46c9-b305-c13e28e8d041",
   "metadata": {},
   "source": [
    "## Ajuste fino de modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a9d00-b6d8-4657-af65-5535ad27b081",
   "metadata": {},
   "source": [
    "Con los datos listos, descargamos el modelo pre-entrenado y lo ajustamos. Dado que todas las tareas a realizar son para clasificación de tokens, usamos la clase ```AutoModelForTokenClassification```. Como con el tokenizado, usamos la función ```from_pretrained``` para descargar y cachear el modelo. Lo único que debemos especificar para el problema es la cantidad de etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f823ad6a-fade-4bc5-9c6f-9521876a70ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b40db08f-b644-4e46-a662-163789cd967a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=batch_size,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "model = DistilBertForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(unique_tags))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79942934-0d0c-43bf-8c22-fc425b8a6ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  6/900 09:50 < 36:40:28, 0.01 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402d90bf-5c8d-4db4-97ce-bef77fc698bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bd1eb2-d2c5-4f75-9d14-b6f54b498489",
   "metadata": {},
   "source": [
    "La advertencia dice que perdemos algunos pesos asociados a las capas de vocabulario y otras inicializadas aleatoriamente (capas del clasificador). Lo anterior es esperado, ya que estamos removiento las capas finales para ajustarlas con los datos que tenemos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faac28e8-fa70-46fe-b60e-d56c8bb5fa7d",
   "metadata": {},
   "source": [
    "Para instanciar un ```Trainer```, necesitamos definir 3 cosas más. La más importante son los ```TrainingArgumnts```, la cual es una clase que contiene todos los atributos para personalizar el entrenamiento. Ésta requiere un nombre de carpeta, el cual se usará para guardar los checkpoints del modelo y los otros argumentos son opcionales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ced6c5-81b2-439b-bb6e-86d42f067499",
   "metadata": {},
   "source": [
    "Aquí vemos que la evaluación se hace en cada epoch, se manipula la tasa de aprendizaje, se usa el tamaño del batch y se define el número de epochs para el entrenamiento, así como el decaimiento de los pesos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac156b84-8d0d-4d44-b8c4-a015969fa565",
   "metadata": {},
   "source": [
    "Luego necesitaremos un ```data_collator``` para que tomen los ejemplos por lotes mientras le aplica padding del mismo tamaño (el padding será del largo del tamaño del más largo ejemplo). Existe un collator para esta tarea en la librería de Transformers, que no sólo aplica el padding sobre las entradas, pero también en las etiquetas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8baab93-a4a1-4378-8250-7f10b12a10c8",
   "metadata": {},
   "source": [
    "Finalmente definimos la métrica sobre la cual se evaluará el entrenamiento, en este caso representada por el framework ```seqeval``` [documentación asociada](https://github.com/chakki-works/seqeval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b0e18c1e-890d-4e36-8208-642834227b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9149b-bc16-44c0-ad72-21ff834c1823",
   "metadata": {},
   "source": [
    "La métrica toma una lista de etiquetas para las predicciones y referencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "be5f1f64-e47c-451e-91be-534ce45c4105",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowTypeError",
     "evalue": "Expected bytes, got a 'list' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-f76dee146dce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# labels = [label_list[i] for i in example[f\"{task}_tags\"]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtag_list\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/datasets/metric.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/datasets/metric.py\u001b[0m in \u001b[0;36madd_batch\u001b[0;34m(self, predictions, references)\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrowInvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36mwrite_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mtyped_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptimizedTypedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtry_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol_try_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mtyped_sequence_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyped_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mpa_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pydict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyped_sequence_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.from_pydict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.asarray\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36m__arrow_array__\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtensionArray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrying_type\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 raise TypeError(\n",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._sequence_to_array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowTypeError\u001b[0m: Expected bytes, got a 'list' object"
     ]
    }
   ],
   "source": [
    "# labels = [label_list[i] for i in example[f\"{task}_tags\"]]\n",
    "labels = [tag_list for tag_list in tags]\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c854e-d21c-4d72-a233-2633991d033a",
   "metadata": {},
   "source": [
    "Necesitaremos post procesar las predicciones\n",
    "\n",
    "* Seleccionar el índice predicho (salida con máximo logit) para cada token\n",
    "* Convertir el token a su etiqueta en string asociada\n",
    "* Ignorar todo lo que tenga la etiqueta -100\n",
    "\n",
    "La siguiente función realiza todo este post procesamiento sobre el resultado ```Trainer.evaluate``` antes de aplicar la métrica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74533418-c39e-435c-b345-54d024e52995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5821b5-a9f1-4a49-8454-0a45e84b1944",
   "metadata": {},
   "source": [
    "Notar que se botan las métricas por etiqueta (la agregaremos luego) y se focalizan sobre los globales. Con lo anterior está listo para entregarlo al ```Trainer```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17833e7b-0c88-4602-b52b-b5b6c33ef028",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b12c73ec-e32f-4fbc-8fe0-37b9254bb92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 8:00:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.210600</td>\n",
       "      <td>0.146080</td>\n",
       "      <td>0.796673</td>\n",
       "      <td>0.774286</td>\n",
       "      <td>0.785320</td>\n",
       "      <td>0.957432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>0.137706</td>\n",
       "      <td>0.812020</td>\n",
       "      <td>0.830790</td>\n",
       "      <td>0.821298</td>\n",
       "      <td>0.962588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.060100</td>\n",
       "      <td>0.143024</td>\n",
       "      <td>0.822331</td>\n",
       "      <td>0.835625</td>\n",
       "      <td>0.828925</td>\n",
       "      <td>0.964022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1563, training_loss=0.11845848412370347, metrics={'train_runtime': 28845.8315, 'train_samples_per_second': 0.054, 'total_flos': 51706117436472.0, 'epoch': 3.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "272392f9-1c62-4f92-8fad-0de2cb5d8843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC': {'precision': 0.8322932917316692,\n",
       "  'recall': 0.8508771929824561,\n",
       "  'f1': 0.8414826498422713,\n",
       "  'number': 1254},\n",
       " 'MISC': {'precision': 0.5792682926829268,\n",
       "  'recall': 0.521978021978022,\n",
       "  'f1': 0.5491329479768786,\n",
       "  'number': 728},\n",
       " 'ORG': {'precision': 0.8139450867052023,\n",
       "  'recall': 0.8553530751708428,\n",
       "  'f1': 0.8341355053683821,\n",
       "  'number': 2634},\n",
       " 'PER': {'precision': 0.9064356435643565,\n",
       "  'recall': 0.9141288067898152,\n",
       "  'f1': 0.9102659706686551,\n",
       "  'number': 2003},\n",
       " 'overall_precision': 0.8223312518584597,\n",
       " 'overall_recall': 0.8356247167245807,\n",
       " 'overall_f1': 0.8289246908954664,\n",
       " 'overall_accuracy': 0.9640221699481197}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd51e7-1edb-4ec5-8f51-dcee99c3d4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
