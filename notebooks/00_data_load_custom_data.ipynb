{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ac57fd-8ec4-4f27-91dc-fedf6714d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable jedi autocompleter\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80225757-b0f7-4f3f-8de9-1df3c9610a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements_custom.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements_custom.txt\n",
    "transformers==4.6.1\n",
    "torch==1.8.1\n",
    "torchvision==0.9.1\n",
    "pandas==1.1.5\n",
    "sklearn==0.0\n",
    "matplotlib==3.4.2\n",
    "ipywidgets==7.6.3\n",
    "datasets==1.6.2\n",
    "seqeval==1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b28edb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.6.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from -r requirements_custom.txt (line 1)) (4.6.1)\n",
      "Requirement already satisfied: torch==1.8.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from -r requirements_custom.txt (line 2)) (1.8.1)\n",
      "Requirement already satisfied: torchvision==0.9.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from -r requirements_custom.txt (line 3)) (0.9.1)\n",
      "Requirement already satisfied: pandas==1.1.5 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from -r requirements_custom.txt (line 4)) (1.1.5)\n",
      "Requirement already satisfied: sklearn==0.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from -r requirements_custom.txt (line 5)) (0.0)\n",
      "Requirement already satisfied: matplotlib==3.4.2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from -r requirements_custom.txt (line 6)) (3.4.2)\n",
      "Requirement already satisfied: ipywidgets==7.6.3 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from -r requirements_custom.txt (line 7)) (7.6.3)\n",
      "Requirement already satisfied: datasets==1.6.2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from -r requirements_custom.txt (line 8)) (1.6.2)\n",
      "Requirement already satisfied: seqeval==1.2.2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from -r requirements_custom.txt (line 9)) (1.2.2)\n",
      "Requirement already satisfied: packaging in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (20.9)\n",
      "Requirement already satisfied: requests in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (2.25.1)\n",
      "Requirement already satisfied: sacremoses in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (0.0.45)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (4.49.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (1.20.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (2021.4.4)\n",
      "Requirement already satisfied: filelock in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (0.0.8)\n",
      "Requirement already satisfied: typing-extensions in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from torch==1.8.1->-r requirements_custom.txt (line 2)) (3.10.0.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from torchvision==0.9.1->-r requirements_custom.txt (line 3)) (8.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from pandas==1.1.5->-r requirements_custom.txt (line 4)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from pandas==1.1.5->-r requirements_custom.txt (line 4)) (2021.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from sklearn==0.0->-r requirements_custom.txt (line 5)) (0.24.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from matplotlib==3.4.2->-r requirements_custom.txt (line 6)) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from matplotlib==3.4.2->-r requirements_custom.txt (line 6)) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from matplotlib==3.4.2->-r requirements_custom.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (7.22.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (3.5.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (5.1.3)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (5.3.4)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (5.0.5)\n",
      "Requirement already satisfied: dill in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from datasets==1.6.2->-r requirements_custom.txt (line 8)) (0.3.3)\n",
      "Requirement already satisfied: fsspec in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from datasets==1.6.2->-r requirements_custom.txt (line 8)) (2021.5.0)\n",
      "Requirement already satisfied: xxhash in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from datasets==1.6.2->-r requirements_custom.txt (line 8)) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from datasets==1.6.2->-r requirements_custom.txt (line 8)) (0.70.11.1)\n",
      "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from datasets==1.6.2->-r requirements_custom.txt (line 8)) (4.0.0)\n",
      "Requirement already satisfied: six in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from cycler>=0.10->matplotlib==3.4.2->-r requirements_custom.txt (line 6)) (1.15.0)\n",
      "Requirement already satisfied: appnope in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.1.2)\n",
      "Requirement already satisfied: jupyter-client in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (6.1.12)\n",
      "Requirement already satisfied: tornado>=4.2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (6.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (4.8.0)\n",
      "Requirement already satisfied: decorator in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (5.0.9)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.17.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (3.0.17)\n",
      "Requirement already satisfied: pygments in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (2.9.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (52.0.0.post20210125)\n",
      "Requirement already satisfied: backcall in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.7.5)\n",
      "Requirement already satisfied: parso>=0.7.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.8.2)\n",
      "Requirement already satisfied: ipython-genutils in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (3.2.0)\n",
      "Requirement already satisfied: jupyter-core in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (4.7.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.17.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.2.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from requests->transformers==4.6.1->-r requirements_custom.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from requests->transformers==4.6.1->-r requirements_custom.txt (line 1)) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from requests->transformers==4.6.1->-r requirements_custom.txt (line 1)) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from requests->transformers==4.6.1->-r requirements_custom.txt (line 1)) (2.10)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from scikit-learn->sklearn==0.0->-r requirements_custom.txt (line 5)) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from scikit-learn->sklearn==0.0->-r requirements_custom.txt (line 5)) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from scikit-learn->sklearn==0.0->-r requirements_custom.txt (line 5)) (1.6.3)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (6.4.0)\n",
      "Requirement already satisfied: nbconvert in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (6.0.7)\n",
      "Requirement already satisfied: argon2-cffi in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (20.1.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (20.0.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.9.4)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.5.0)\n",
      "Requirement already satisfied: prometheus-client in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.10.1)\n",
      "Requirement already satisfied: jinja2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (3.0.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0.0rc2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (2.0.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.4.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.8.4)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.5.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.1.2)\n",
      "Requirement already satisfied: bleach in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (3.3.0)\n",
      "Requirement already satisfied: defusedxml in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.7.1)\n",
      "Requirement already satisfied: testpath in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.4.4)\n",
      "Requirement already satisfied: nest-asyncio in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.5.1)\n",
      "Requirement already satisfied: async-generator in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.10)\n",
      "Requirement already satisfied: webencodings in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.5.1)\n",
      "Requirement already satisfied: click in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from sacremoses->transformers==4.6.1->-r requirements_custom.txt (line 1)) (8.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements_custom.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11796a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd7cc3a-b80b-4b75-a9f3-74bab9f77eb5",
   "metadata": {},
   "source": [
    "# NER BETO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42380dac-9cd7-4f19-b6bf-e6283bc891b2",
   "metadata": {},
   "source": [
    "A modo de ejemplo, usaremos la implementaci칩n mostrada en [Token Classification](https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb), ajustada al caso de BETO [publicaci칩n asociada](https://users.dcc.uchile.cl/~jperez/papers/pml4dc2020.pdf). Otro ejemplo interesante a revisar es [TinyBERT](https://huggingface.co/mrm8488/TinyBERT-spanish-uncased-finetuned-ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73cff9bb-c77d-4420-af57-ceb1afbaf0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from transformers import BertForMaskedLM, BertTokenizer\n",
    "# from transformers import BertForTokenClassification\n",
    "from transformers import DistilBertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8abce13b-9a1a-46a8-8734-5a1672a8a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"distilbert-base-cased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85b41da-cd00-4b8f-9def-f72182df487b",
   "metadata": {},
   "source": [
    "Definimos el dispositivo sobre el cual se har치 entrenamiento, en caso de tener disponible una GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10e4d762-22a3-4562-8e59-fdbd64bca4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "376bb99c-3493-43d1-a3a6-71cb0a86cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertTokenizerFast.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bb6ba8-3b4c-409b-b84b-43fcd2b93389",
   "metadata": {},
   "source": [
    "## Carga de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59bc8824-0e19-47b1-9a3c-92e59172e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "767b7386-e97c-4226-8ec1-bd6be42c0d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/iacastro/PycharmProjects/ner_tests/data/raw/archive/ner_dataset.csv'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_folder = os.getcwd().split('ner_tests')[0]\n",
    "project_folder = '{}ner_tests'.format(root_folder)\n",
    "# project_folder\n",
    "data_path = '{}/data/raw/archive/ner_dataset.csv'.format(project_folder)\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0b2b6b5-ba52-4e1f-8718-cd50c894ce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner = pd.read_csv(data_path,delimiter=',',encoding='latin1',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1e5d117-bbea-41ba-9620-f72cda224e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner['Sentence #'] = df_ner['Sentence #'].ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff56b994-7c09-4760-b636-728181619333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demand</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>British</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>troops</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>from</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>that</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>country</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Families</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>soldiers</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>killed</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>conflict</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>joined</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>protesters</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>who</td>\n",
       "      <td>WP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>carried</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>banners</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>with</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>such</td>\n",
       "      <td>JJ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>slogans</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>as</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>\"</td>\n",
       "      <td>``</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Bush</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-per</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Number</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>One</td>\n",
       "      <td>CD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Terrorist</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>\"</td>\n",
       "      <td>``</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>\"</td>\n",
       "      <td>``</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Stop</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #           Word  POS    Tag\n",
       "0   Sentence: 1      Thousands  NNS      O\n",
       "1   Sentence: 1             of   IN      O\n",
       "2   Sentence: 1  demonstrators  NNS      O\n",
       "3   Sentence: 1           have  VBP      O\n",
       "4   Sentence: 1        marched  VBN      O\n",
       "5   Sentence: 1        through   IN      O\n",
       "6   Sentence: 1         London  NNP  B-geo\n",
       "7   Sentence: 1             to   TO      O\n",
       "8   Sentence: 1        protest   VB      O\n",
       "9   Sentence: 1            the   DT      O\n",
       "10  Sentence: 1            war   NN      O\n",
       "11  Sentence: 1             in   IN      O\n",
       "12  Sentence: 1           Iraq  NNP  B-geo\n",
       "13  Sentence: 1            and   CC      O\n",
       "14  Sentence: 1         demand   VB      O\n",
       "15  Sentence: 1            the   DT      O\n",
       "16  Sentence: 1     withdrawal   NN      O\n",
       "17  Sentence: 1             of   IN      O\n",
       "18  Sentence: 1        British   JJ  B-gpe\n",
       "19  Sentence: 1         troops  NNS      O\n",
       "20  Sentence: 1           from   IN      O\n",
       "21  Sentence: 1           that   DT      O\n",
       "22  Sentence: 1        country   NN      O\n",
       "23  Sentence: 1              .    .      O\n",
       "24  Sentence: 2       Families  NNS      O\n",
       "25  Sentence: 2             of   IN      O\n",
       "26  Sentence: 2       soldiers  NNS      O\n",
       "27  Sentence: 2         killed  VBN      O\n",
       "28  Sentence: 2             in   IN      O\n",
       "29  Sentence: 2            the   DT      O\n",
       "30  Sentence: 2       conflict   NN      O\n",
       "31  Sentence: 2         joined  VBD      O\n",
       "32  Sentence: 2            the   DT      O\n",
       "33  Sentence: 2     protesters  NNS      O\n",
       "34  Sentence: 2            who   WP      O\n",
       "35  Sentence: 2        carried  VBD      O\n",
       "36  Sentence: 2        banners  NNS      O\n",
       "37  Sentence: 2           with   IN      O\n",
       "38  Sentence: 2           such   JJ      O\n",
       "39  Sentence: 2        slogans  NNS      O\n",
       "40  Sentence: 2             as   IN      O\n",
       "41  Sentence: 2              \"   ``      O\n",
       "42  Sentence: 2           Bush  NNP  B-per\n",
       "43  Sentence: 2         Number   NN      O\n",
       "44  Sentence: 2            One   CD      O\n",
       "45  Sentence: 2      Terrorist   NN      O\n",
       "46  Sentence: 2              \"   ``      O\n",
       "47  Sentence: 2            and   CC      O\n",
       "48  Sentence: 2              \"   ``      O\n",
       "49  Sentence: 2           Stop   VB      O"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ner.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99f66ef6-ad5d-4278-a1dd-68a26824c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_texts = df_ner['Sentence #'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcd49330-dd3e-46e6-b80d-06831e622f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df_ner.groupby('Sentence #')['Word'].apply(list).to_list()#.groupby(level=0).apply(list)\n",
    "tags = df_ner.groupby('Sentence #')['Tag'].apply(list).to_list()#.groupby(level=0).apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d92c9160-7f01-48b4-93b9-277827c6ce82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['war', 'in', 'Iraq', 'and', 'demand', 'the', 'withdrawal']\n",
      "['O', 'O', 'B-geo', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(texts[0][10:17], tags[0][10:17], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1e723bb-f8bc-4064-b2a8-dcfbdd2671db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb58e8c-562f-4fbe-bbed-860b85c2f429",
   "metadata": {},
   "source": [
    "Generaremos encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ce43908-e862-40bd-bdbb-8c0137dd5570",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = set(tag for doc in tags for tag in doc)\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c624e354-60e9-4943-b194-8cff1eadb1ab",
   "metadata": {},
   "source": [
    "Se toman pads para tokenizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26c2d465-82ad-4da3-86db-041d1410f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_checkpoint)\n",
    "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb0c4e-eb64-47ec-ac6e-93d3431b9faa",
   "metadata": {},
   "source": [
    "Se codifican los tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0aa5d70-86a8-48ee-891f-2dd8379d3a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def encode_tags(tags, encodings):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    count = 0\n",
    "    error_encodings = []\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "        # print(count)\n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "        # print(arr_offset[:,0])\n",
    "        try:\n",
    "            doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "            encoded_labels.append(doc_enc_labels.tolist())\n",
    "        except:\n",
    "            error_encodings.append(count)\n",
    "        count += 1\n",
    "\n",
    "    return encoded_labels,error_encodings\n",
    "\n",
    "train_labels, train_errors = encode_tags(train_tags, train_encodings)\n",
    "val_labels, val_errors = encode_tags(val_tags, val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adfb45ae-5c47-4915-b1a0-311dab422234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[340,\n",
       " 4077,\n",
       " 12605,\n",
       " 14686,\n",
       " 15798,\n",
       " 16941,\n",
       " 22161,\n",
       " 22350,\n",
       " 23028,\n",
       " 24107,\n",
       " 25383,\n",
       " 28715,\n",
       " 28828,\n",
       " 31176,\n",
       " 31703,\n",
       " 33963,\n",
       " 34123,\n",
       " 35047]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9f58744-d977-4d15-ba4c-276c1b57375e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[699, 4864, 5388]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "351dae01-2314-4d40-873d-8aac96dbd38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
    "val_encodings.pop(\"offset_mapping\")\n",
    "train_dataset = NERDataset(train_encodings, train_labels)\n",
    "val_dataset = NERDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f133b-5807-46c9-b305-c13e28e8d041",
   "metadata": {},
   "source": [
    "## Ajuste fino de modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a9d00-b6d8-4657-af65-5535ad27b081",
   "metadata": {},
   "source": [
    "Con los datos listos, descargamos el modelo pre-entrenado y lo ajustamos. Dado que todas las tareas a realizar son para clasificaci칩n de tokens, usamos la clase ```AutoModelForTokenClassification```. Como con el tokenizado, usamos la funci칩n ```from_pretrained``` para descargar y cachear el modelo. Lo 칰nico que debemos especificar para el problema es la cantidad de etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f823ad6a-fade-4bc5-9c6f-9521876a70ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b40db08f-b644-4e46-a662-163789cd967a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=batch_size,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "model = DistilBertForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(unique_tags))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 游뱅 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79942934-0d0c-43bf-8c22-fc425b8a6ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  6/900 09:50 < 36:40:28, 0.01 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402d90bf-5c8d-4db4-97ce-bef77fc698bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bd1eb2-d2c5-4f75-9d14-b6f54b498489",
   "metadata": {},
   "source": [
    "La advertencia dice que perdemos algunos pesos asociados a las capas de vocabulario y otras inicializadas aleatoriamente (capas del clasificador). Lo anterior es esperado, ya que estamos removiento las capas finales para ajustarlas con los datos que tenemos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faac28e8-fa70-46fe-b60e-d56c8bb5fa7d",
   "metadata": {},
   "source": [
    "Para instanciar un ```Trainer```, necesitamos definir 3 cosas m치s. La m치s importante son los ```TrainingArgumnts```, la cual es una clase que contiene todos los atributos para personalizar el entrenamiento. 칄sta requiere un nombre de carpeta, el cual se usar치 para guardar los checkpoints del modelo y los otros argumentos son opcionales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ced6c5-81b2-439b-bb6e-86d42f067499",
   "metadata": {},
   "source": [
    "Aqu칤 vemos que la evaluaci칩n se hace en cada epoch, se manipula la tasa de aprendizaje, se usa el tama침o del batch y se define el n칰mero de epochs para el entrenamiento, as칤 como el decaimiento de los pesos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac156b84-8d0d-4d44-b8c4-a015969fa565",
   "metadata": {},
   "source": [
    "Luego necesitaremos un ```data_collator``` para que tomen los ejemplos por lotes mientras le aplica padding del mismo tama침o (el padding ser치 del largo del tama침o del m치s largo ejemplo). Existe un collator para esta tarea en la librer칤a de Transformers, que no s칩lo aplica el padding sobre las entradas, pero tambi칠n en las etiquetas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8baab93-a4a1-4378-8250-7f10b12a10c8",
   "metadata": {},
   "source": [
    "Finalmente definimos la m칠trica sobre la cual se evaluar치 el entrenamiento, en este caso representada por el framework ```seqeval``` [documentaci칩n asociada](https://github.com/chakki-works/seqeval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b0e18c1e-890d-4e36-8208-642834227b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9149b-bc16-44c0-ad72-21ff834c1823",
   "metadata": {},
   "source": [
    "La m칠trica toma una lista de etiquetas para las predicciones y referencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "be5f1f64-e47c-451e-91be-534ce45c4105",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowTypeError",
     "evalue": "Expected bytes, got a 'list' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowTypeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-f76dee146dce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# labels = [label_list[i] for i in example[f\"{task}_tags\"]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtag_list\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/datasets/metric.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/datasets/metric.py\u001b[0m in \u001b[0;36madd_batch\u001b[0;34m(self, predictions, references)\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrowInvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36mwrite_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mtyped_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptimizedTypedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtry_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol_try_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mtyped_sequence_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyped_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mpa_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pydict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyped_sequence_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.from_pydict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.asarray\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36m__arrow_array__\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtensionArray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrying_type\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 raise TypeError(\n",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._sequence_to_array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ner_tests/lib/python3.8/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowTypeError\u001b[0m: Expected bytes, got a 'list' object"
     ]
    }
   ],
   "source": [
    "# labels = [label_list[i] for i in example[f\"{task}_tags\"]]\n",
    "labels = [tag_list for tag_list in tags]\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c854e-d21c-4d72-a233-2633991d033a",
   "metadata": {},
   "source": [
    "Necesitaremos post procesar las predicciones\n",
    "\n",
    "* Seleccionar el 칤ndice predicho (salida con m치ximo logit) para cada token\n",
    "* Convertir el token a su etiqueta en string asociada\n",
    "* Ignorar todo lo que tenga la etiqueta -100\n",
    "\n",
    "La siguiente funci칩n realiza todo este post procesamiento sobre el resultado ```Trainer.evaluate``` antes de aplicar la m칠trica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74533418-c39e-435c-b345-54d024e52995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5821b5-a9f1-4a49-8454-0a45e84b1944",
   "metadata": {},
   "source": [
    "Notar que se botan las m칠tricas por etiqueta (la agregaremos luego) y se focalizan sobre los globales. Con lo anterior est치 listo para entregarlo al ```Trainer```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17833e7b-0c88-4602-b52b-b5b6c33ef028",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b12c73ec-e32f-4fbc-8fe0-37b9254bb92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 8:00:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.210600</td>\n",
       "      <td>0.146080</td>\n",
       "      <td>0.796673</td>\n",
       "      <td>0.774286</td>\n",
       "      <td>0.785320</td>\n",
       "      <td>0.957432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>0.137706</td>\n",
       "      <td>0.812020</td>\n",
       "      <td>0.830790</td>\n",
       "      <td>0.821298</td>\n",
       "      <td>0.962588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.060100</td>\n",
       "      <td>0.143024</td>\n",
       "      <td>0.822331</td>\n",
       "      <td>0.835625</td>\n",
       "      <td>0.828925</td>\n",
       "      <td>0.964022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1563, training_loss=0.11845848412370347, metrics={'train_runtime': 28845.8315, 'train_samples_per_second': 0.054, 'total_flos': 51706117436472.0, 'epoch': 3.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "272392f9-1c62-4f92-8fad-0de2cb5d8843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC': {'precision': 0.8322932917316692,\n",
       "  'recall': 0.8508771929824561,\n",
       "  'f1': 0.8414826498422713,\n",
       "  'number': 1254},\n",
       " 'MISC': {'precision': 0.5792682926829268,\n",
       "  'recall': 0.521978021978022,\n",
       "  'f1': 0.5491329479768786,\n",
       "  'number': 728},\n",
       " 'ORG': {'precision': 0.8139450867052023,\n",
       "  'recall': 0.8553530751708428,\n",
       "  'f1': 0.8341355053683821,\n",
       "  'number': 2634},\n",
       " 'PER': {'precision': 0.9064356435643565,\n",
       "  'recall': 0.9141288067898152,\n",
       "  'f1': 0.9102659706686551,\n",
       "  'number': 2003},\n",
       " 'overall_precision': 0.8223312518584597,\n",
       " 'overall_recall': 0.8356247167245807,\n",
       " 'overall_f1': 0.8289246908954664,\n",
       " 'overall_accuracy': 0.9640221699481197}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd51e7-1edb-4ec5-8f51-dcee99c3d4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
