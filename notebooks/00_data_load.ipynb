{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b3a48f-8b47-4549-a387-da7d2ed2d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable jedi autocompleter\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80225757-b0f7-4f3f-8de9-1df3c9610a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements_custom.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements_custom.txt\n",
    "transformers==4.6.1\n",
    "torch==1.8.1\n",
    "torchvision==0.9.1\n",
    "pandas==1.1.5\n",
    "sklearn==0.0\n",
    "matplotlib==3.4.2\n",
    "ipywidgets==7.6.3\n",
    "datasets==1.6.2\n",
    "seqeval==1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b28edb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.6.1\n",
      "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch==1.8.1\n",
      "  Downloading torch-1.8.1-cp38-none-macosx_10_9_x86_64.whl (119.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 119.6 MB 1.9 MB/s eta 0:00:01     |████████████████████▊           | 77.5 MB 943 kB/s eta 0:00:45     |███████████████████████▍        | 87.3 MB 2.4 MB/s eta 0:00:14\n",
      "\u001b[?25hCollecting torchvision==0.9.1\n",
      "  Downloading torchvision-0.9.1-cp38-cp38-macosx_10_9_x86_64.whl (13.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.2 MB 925 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas==1.1.5\n",
      "  Downloading pandas-1.1.5-cp38-cp38-macosx_10_9_x86_64.whl (10.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.1 MB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sklearn==0.0\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting matplotlib==3.4.2\n",
      "  Downloading matplotlib-3.4.2-cp38-cp38-macosx_10_9_x86_64.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 370 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ipywidgets==7.6.3\n",
      "  Downloading ipywidgets-7.6.3-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting datasets==1.6.2\n",
      "  Downloading datasets-1.6.2-py3-none-any.whl (221 kB)\n",
      "\u001b[K     |████████████████████████████████| 221 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting seqeval==1.2.2\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 91 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: packaging in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (20.9)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp38-cp38-macosx_10_11_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 348 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2021.4.4-cp38-cp38-macosx_10_9_x86_64.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.17\n",
      "  Downloading numpy-1.20.3-cp38-cp38-macosx_10_9_x86_64.whl (16.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 16.0 MB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (2.25.1)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.61.0-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 2.1 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-3.10.0.0-py3-none-any.whl (26 kB)\n",
      "Collecting pillow>=4.1.1\n",
      "  Downloading Pillow-8.2.0-cp38-cp38-macosx_10_10_x86_64.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from pandas==1.1.5->-r requirements_custom.txt (line 4)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from pandas==1.1.5->-r requirements_custom.txt (line 4)) (2021.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.2-cp38-cp38-macosx_10_13_x86_64.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from matplotlib==3.4.2->-r requirements_custom.txt (line 6)) (2.4.7)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp38-cp38-macosx_10_9_x86_64.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 456 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jupyterlab-widgets>=1.0.0\n",
      "  Downloading jupyterlab_widgets-1.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nbformat>=4.2.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (5.1.3)\n",
      "Collecting widgetsnbextension~=3.5.0\n",
      "  Downloading widgetsnbextension-3.5.1-py2.py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 887 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: traitlets>=4.3.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (5.0.5)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (5.3.4)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (7.22.0)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.11.1-py38-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow>=1.0.0<4.0.0\n",
      "  Downloading pyarrow-4.0.0-cp38-cp38-macosx_10_13_x86_64.whl (15.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7 MB 781 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 2.4 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp38-cp38-macosx_10_9_x86_64.whl (31 kB)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2021.5.0-py3-none-any.whl (111 kB)\n",
      "\u001b[K     |████████████████████████████████| 111 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from cycler>=0.10->matplotlib==3.4.2->-r requirements_custom.txt (line 6)) (1.15.0)\n",
      "Requirement already satisfied: appnope in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.1.2)\n",
      "Requirement already satisfied: jupyter-client in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (6.1.12)\n",
      "Requirement already satisfied: tornado>=4.2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (6.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pygments in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (2.9.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.17.0)\n",
      "Requirement already satisfied: pickleshare in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (3.0.17)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (4.8.0)\n",
      "Requirement already satisfied: backcall in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: decorator in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (5.0.9)\n",
      "Requirement already satisfied: parso>=0.7.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.8.2)\n",
      "Requirement already satisfied: ipython-genutils in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (4.7.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (3.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (21.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.2.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from requests->transformers==4.6.1->-r requirements_custom.txt (line 1)) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from requests->transformers==4.6.1->-r requirements_custom.txt (line 1)) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from requests->transformers==4.6.1->-r requirements_custom.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from requests->transformers==4.6.1->-r requirements_custom.txt (line 1)) (4.0.0)\n",
      "Collecting scipy>=0.19.1\n",
      "  Downloading scipy-1.6.3-cp38-cp38-macosx_10_9_x86_64.whl (30.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 30.8 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib>=0.11\n",
      "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "\u001b[K     |████████████████████████████████| 303 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (6.4.0)\n",
      "Requirement already satisfied: prometheus-client in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.10.1)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.9.4)\n",
      "Requirement already satisfied: argon2-cffi in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (20.1.0)\n",
      "Requirement already satisfied: jinja2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (3.0.0)\n",
      "Requirement already satisfied: nbconvert in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (6.0.7)\n",
      "Requirement already satisfied: pyzmq>=17 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (20.0.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0.0rc2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (2.0.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.4.3)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.5.3)\n",
      "Requirement already satisfied: bleach in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (3.3.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.3)\n",
      "Requirement already satisfied: defusedxml in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.7.1)\n",
      "Requirement already satisfied: testpath in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.4.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.1.2)\n",
      "Requirement already satisfied: nest-asyncio in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.5.1)\n",
      "Requirement already satisfied: async-generator in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.10)\n",
      "Requirement already satisfied: webencodings in /Users/iacastro/.conda/envs/ner_tests/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.5.1)\n",
      "Collecting click\n",
      "  Using cached click-8.0.1-py3-none-any.whl (97 kB)\n",
      "Building wheels for collected packages: sklearn, seqeval\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=4c87663e479302988ca9cccdd441463ae06333968fca98016240eadb530052ee\n",
      "  Stored in directory: /Users/iacastro/Library/Caches/pip/wheels/22/0b/40/fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16170 sha256=c248c23586abe69a83018ae38e7041116b99e3407b48450c62aae07376e1edb8\n",
      "  Stored in directory: /Users/iacastro/Library/Caches/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
      "Successfully built sklearn seqeval\n",
      "Installing collected packages: numpy, typing-extensions, tqdm, threadpoolctl, scipy, regex, joblib, filelock, dill, click, xxhash, widgetsnbextension, torch, tokenizers, scikit-learn, sacremoses, pyarrow, pillow, pandas, multiprocess, kiwisolver, jupyterlab-widgets, huggingface-hub, fsspec, cycler, transformers, torchvision, sklearn, seqeval, matplotlib, ipywidgets, datasets\n",
      "Successfully installed click-8.0.1 cycler-0.10.0 datasets-1.6.2 dill-0.3.3 filelock-3.0.12 fsspec-2021.5.0 huggingface-hub-0.0.8 ipywidgets-7.6.3 joblib-1.0.1 jupyterlab-widgets-1.0.0 kiwisolver-1.3.1 matplotlib-3.4.2 multiprocess-0.70.11.1 numpy-1.20.3 pandas-1.1.5 pillow-8.2.0 pyarrow-4.0.0 regex-2021.4.4 sacremoses-0.0.45 scikit-learn-0.24.2 scipy-1.6.3 seqeval-1.2.2 sklearn-0.0 threadpoolctl-2.1.0 tokenizers-0.10.3 torch-1.8.1 torchvision-0.9.1 tqdm-4.49.0 transformers-4.6.1 typing-extensions-3.10.0.0 widgetsnbextension-3.5.1 xxhash-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements_custom.txt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a751f925-673d-49a2-9c4f-f7662855d738",
   "metadata": {},
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11796a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd7cc3a-b80b-4b75-a9f3-74bab9f77eb5",
   "metadata": {},
   "source": [
    "# NER BETO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42380dac-9cd7-4f19-b6bf-e6283bc891b2",
   "metadata": {},
   "source": [
    "A modo de ejemplo, usaremos la implementación mostrada en [Token Classification](https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb), ajustada al caso de BETO [publicación asociada](https://users.dcc.uchile.cl/~jperez/papers/pml4dc2020.pdf). Otro ejemplo interesante a revisar es [TinyBERT](https://huggingface.co/mrm8488/TinyBERT-spanish-uncased-finetuned-ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73cff9bb-c77d-4420-af57-ceb1afbaf0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from transformers import BertForMaskedLM, BertTokenizer\n",
    "from transformers import BertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8abce13b-9a1a-46a8-8734-5a1672a8a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85b41da-cd00-4b8f-9def-f72182df487b",
   "metadata": {},
   "source": [
    "Definimos el dispositivo sobre el cual se hará entrenamiento, en caso de tener disponible una GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10e4d762-22a3-4562-8e59-fdbd64bca4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "376bb99c-3493-43d1-a3a6-71cb0a86cc2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0591476c48e443c1b4f25387ad85d9b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=650.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e66e44733f476699d61b04950384d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=439621341.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f738ece8-1f02-44da-bd11-0117f5ac0b8e",
   "metadata": {},
   "source": [
    "Podemos pasar el modelo al dispositivo asociado, y ver la arquitectura pre-entrenada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0acaa71f-f701-458e-8cba-3575256076aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bb6ba8-3b4c-409b-b84b-43fcd2b93389",
   "metadata": {},
   "source": [
    "## Carga de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a85cb83-8c37-488f-9535-09e6a7fa1513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric, list_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55027fe5-06df-4ea7-8a40-35ed6c897358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acronym_identification, ade_corpus_v2, adversarial_qa, aeslc, afrikaans_ner_corpus, ag_news, ai2_arc, air_dialogue, ajgt_twitter_ar, allegro_reviews, allocine, alt, amazon_polarity, amazon_reviews_multi, amazon_us_reviews, ambig_qa, amttl, anli, app_reviews, aqua_rat, aquamuse, ar_cov19, ar_res_reviews, ar_sarcasm, arabic_billion_words, arabic_pos_dialect, arabic_speech_corpus, arcd, arsentd_lev, art, arxiv_dataset, ascent_kb, aslg_pc12, asnq, asset, assin, assin2, atomic, autshumato, babi_qa, banking77, bbaw_egyptian, bbc_hindi_nli, bc2gm_corpus, best2009, bianet, bible_para, big_patent, billsum, bing_coronavirus_query_set, biomrc, blended_skill_talk, blimp, blog_authorship_corpus, bn_hate_speech, bookcorpus, bookcorpusopen, boolq, bprec, break_data, brwac, bsd_ja_en, bswac, c3, c4, cail2018, caner, capes, catalonia_independence, cawac, cbt, cc100, cc_news, ccaligned_multilingual, cdsc, cdt, cfq, chr_en, cifar10, cifar100, circa, civil_comments, clickbait_news_bg, climate_fever, clinc_oos, clue, cmrc2018, cnn_dailymail, coached_conv_pref, coarse_discourse, codah, code_search_net, com_qa, common_gen, common_voice, commonsense_qa, compguesswhat, conceptnet5, conll2000, conll2002, conll2003, conllpp, conv_ai, conv_ai_2, conv_ai_3, conv_questions, coqa, cord19, cornell_movie_dialog, cos_e, cosmos_qa, counter, covid_qa_castorini, covid_qa_deepset, covid_qa_ucsd, covid_tweets_japanese, covost2, craigslist_bargains, crawl_domain, crd3, crime_and_punish, crows_pairs, cryptonite, cs_restaurants, cuad, curiosity_dialogs, daily_dialog, dane, danish_political_comments, dart, datacommons_factcheck, dbpedia_14, dbrd, deal_or_no_dialog, definite_pronoun_resolution, dengue_filipino, dialog_re, diplomacy_detection, disaster_response_messages, discofuse, discovery, doc2dial, docred, doqa, dream, drop, duorc, dutch_social, dyk, e2e_nlg, e2e_nlg_cleaned, ecb, ecthr_cases, ehealth_kd, eitb_parcc, eli5, emea, emo, emotion, emotone_ar, empathetic_dialogues, enriched_web_nlg, eraser_multi_rc, esnli, eth_py150_open, ethos, eu_regulatory_ir, eurlex, euronews, europa_eac_tm, europa_ecdc_tm, europarl_bilingual, event2Mind, evidence_infer_treatment, exams, factckbr, fake_news_english, fake_news_filipino, farsi_news, fashion_mnist, fever, few_rel, financial_phrasebank, finer, flores, flue, fquad, freebase_qa, gap, gem, generated_reviews_enth, generics_kb, german_legal_entity_recognition, germaner, germeval_14, giga_fren, gigaword, glucose, glue, gnad10, go_emotions, gooaq, google_wellformed_query, grail_qa, great_code, guardian_authorship, gutenberg_time, hans, hansards, hard, harem, has_part, hate_offensive, hate_speech18, hate_speech_filipino, hate_speech_offensive, hate_speech_pl, hate_speech_portuguese, hatexplain, hausa_voa_ner, hausa_voa_topics, hda_nli_hindi, head_qa, health_fact, hebrew_projectbenyehuda, hebrew_sentiment, hebrew_this_world, hellaswag, hind_encorp, hindi_discourse, hippocorpus, hkcancor, hlgd, hope_edi, hotpot_qa, hover, hrenwac_para, hrwac, humicroedit, hybrid_qa, hyperpartisan_news_detection, iapp_wiki_qa_squad, id_clickbait, id_liputan6, id_nergrit_corpus, id_newspapers_2018, id_panl_bppt, id_puisi, igbo_english_machine_translation, igbo_monolingual, igbo_ner, ilist, imdb, imdb_urdu_reviews, imppres, indic_glue, indonlu, inquisitive_qg, interpress_news_category_tr, interpress_news_category_tr_lite, irc_disentangle, isixhosa_ner_corpus, isizulu_ner_corpus, iwslt2017, jeopardy, jfleg, jigsaw_toxicity_pred, jnlpba, journalists_questions, kannada_news, kd_conv, kde4, kelm, kilt_tasks, kilt_wikipedia, kinnews_kirnews, kor_3i4k, kor_hate, kor_ner, kor_nli, kor_nlu, kor_qpair, kor_sae, kor_sarcasm, labr, lama, lambada, large_spanish_corpus, laroseda, lc_quad, lener_br, liar, librispeech_asr, librispeech_lm, limit, lince, linnaeus, liveqa, lj_speech, lm1b, lst20, m_lama, mac_morpho, makhzan, math_dataset, math_qa, matinf, mc_taco, md_gender_bias, mdd, med_hop, medal, medical_dialog, medical_questions_pairs, menyo20k_mt, meta_woz, metooma, metrec, miam, mkb, mkqa, mlqa, mlsum, mnist, mocha, moroco, movie_rationales, mrqa, ms_marco, ms_terms, msr_genomics_kbcomp, msr_sqa, msr_text_compression, msr_zhen_translation_parity, msra_ner, mt_eng_vietnamese, muchocine, multi_booked, multi_news, multi_nli, multi_nli_mismatch, multi_para_crawl, multi_re_qa, multi_woz_v22, multi_x_science_sum, mutual_friends, mwsc, myanmar_news, narrativeqa, narrativeqa_manual, natural_questions, ncbi_disease, nchlt, ncslgr, nell, neural_code_search, news_commentary, newsgroup, newsph, newsph_nli, newspop, newsqa, newsroom, nkjp-ner, nli_tr, nlu_evaluation_data, norec, norne, norwegian_ner, nq_open, nsmc, numer_sense, numeric_fused_head, oclar, offcombr, offenseval2020_tr, offenseval_dravidian, ofis_publik, ohsumed, ollie, omp, onestop_english, open_subtitles, openbookqa, openslr, openwebtext, opinosis, opus100, opus_books, opus_dgt, opus_dogc, opus_elhuyar, opus_euconst, opus_finlex, opus_fiskmo, opus_gnome, opus_infopankki, opus_memat, opus_montenegrinsubs, opus_openoffice, opus_paracrawl, opus_rf, opus_tedtalks, opus_ubuntu, opus_wikipedia, opus_xhosanavy, orange_sum, oscar, para_crawl, para_pat, parsinlu_reading_comprehension, paws, paws-x, pec, peer_read, peoples_daily_ner, per_sent, persian_ner, pg19, php, piaf, pib, piqa, pn_summary, poem_sentiment, polemo2, poleval2019_cyberbullying, poleval2019_mt, polsum, polyglot_ner, prachathai67k, pragmeval, proto_qa, psc, ptb_text_only, pubmed, pubmed_qa, py_ast, qa4mre, qa_srl, qa_zre, qangaroo, qanta, qasc, qasper, qed, qed_amara, quac, quail, quarel, quartz, quora, quoref, race, re_dial, reasoning_bg, recipe_nlg, reclor, reddit, reddit_tifu, refresd, reuters21578, ro_sent, ro_sts, ro_sts_parallel, roman_urdu, ronec, ropes, rotten_tomatoes, s2orc, samsum, sanskrit_classic, saudinewsnet, scan, scb_mt_enth_2020, schema_guided_dstc8, scicite, scielo, scientific_papers, scifact, sciq, scitail, scitldr, search_qa, selqa, sem_eval_2010_task_8, sem_eval_2014_task_1, sem_eval_2020_task_11, sent_comp, senti_lex, senti_ws, sentiment140, sepedi_ner, sesotho_ner_corpus, setimes, setswana_ner_corpus, sharc, sharc_modified, sick, silicone, simple_questions_v2, siswati_ner_corpus, smartdata, sms_spam, snips_built_in_intents, snli, snow_simplified_japanese_corpus, so_stacksample, social_bias_frames, social_i_qa, sofc_materials_articles, sogou_news, spanish_billion_words, spc, species_800, spider, squad, squad_adversarial, squad_es, squad_it, squad_kor_v1, squad_kor_v2, squad_v1_pt, squad_v2, squadshifts, srwac, sst, stereoset, stsb_mt_sv, stsb_multi_mt, style_change_detection, subjqa, super_glue, swag, swahili, swahili_news, swda, swedish_ner_corpus, swedish_reviews, tab_fact, tamilmixsentiment, tanzil, tapaco, tashkeela, taskmaster1, taskmaster2, taskmaster3, tatoeba, ted_hrlr, ted_iwlst2013, ted_multi, ted_talks_iwslt, telugu_books, telugu_news, tep_en_fa_para, thai_toxicity_tweet, thainer, thaiqa_squad, thaisum, tilde_model, times_of_india_news_headlines, timit_asr, tiny_shakespeare, tlc, tmu_gfm_dataset, totto, trec, trivia_qa, tsac, ttc4900, tunizi, tuple_ie, turk, turkish_movie_sentiment, turkish_ner, turkish_product_reviews, turkish_shrinked_ner, turku_ner_corpus, tweet_eval, tweet_qa, tweets_ar_en_parallel, tweets_hate_speech_detection, twi_text_c3, twi_wordsim353, tydiqa, ubuntu_dialogs_corpus, udhr, um005, un_ga, un_multi, un_pc, universal_dependencies, universal_morphologies, urdu_fake_news, urdu_sentiment_corpus, web_nlg, web_of_science, web_questions, weibo_ner, wi_locness, wiki40b, wiki_asp, wiki_atomic_edits, wiki_auto, wiki_bio, wiki_dpr, wiki_hop, wiki_lingua, wiki_movies, wiki_qa, wiki_qa_ar, wiki_snippets, wiki_source, wiki_split, wiki_summary, wikiann, wikicorpus, wikihow, wikipedia, wikisql, wikitext, wikitext_tl39, wili_2018, wino_bias, winograd_wsc, winogrande, wiqa, wisesight1000, wisesight_sentiment, wmt14, wmt15, wmt16, wmt17, wmt18, wmt19, wmt20_mlqe_task1, wmt20_mlqe_task2, wmt20_mlqe_task3, wmt_t2t, wnut_17, wongnai_reviews, woz_dialogue, wrbsc, x_stance, xcopa, xed_en_fi, xglue, xnli, xor_tydi_qa, xquad, xquad_r, xsum, xsum_factuality, xtreme, yahoo_answers_qa, yahoo_answers_topics, yelp_polarity, yelp_review_full, yoruba_bbc_topics, yoruba_gv_ner, yoruba_text_c3, yoruba_wordsim353, youtube_caption_corrections, zest, AConsApart/anime_subtitles_DialoGPT, AdWeeb/DravidianMT, Adnan/Urdu_News_Headlines, Akshith/aa, Akshith/g_rock, Akshith/test, Annielytics/DoctorsNotes, Avishekavi/Avi, Binbin/my_dataset, Darren/data, EMBO/biolang, EMBO/sd-nlp, Eymen3455/xsum_tr, FRTNX/cosuju, Firoj/CrisisBench, Fraser/mnist-text-default, Fraser/mnist-text-no-spaces, Fraser/mnist-text-small, Fraser/news-category-dataset, Fraser/python-lines, Fraser/short-jokes, Halilyesilceng/autonlp-data-nameEntityRecognition, Harveenchadha/Gujarati-Monolingual-Data, Jean-Baptiste/wikiner_fr, LIAMF-USP/arc-retrieval-c4, MKK/Dhivehi-English, MarianaSahagun/test, Melinoe/TheLabTexts, NTUYG/RAGTest, NbAiLab/norec_agg, NbAiLab/norne, NbAiLab/norwegian_parliament, Ofrit/tmp, QA/abk-eng, Remesita/tagged_reviews, SajjadAyoubi/persian_qa, TRoboto/masc, Terry0107/RiSAWOZ, TimTreasure4/Test, Trainmaster9977/957, Trainmaster9977/zbakuman, Tyler/wikimatrix_collapsed, Valahaar/wsdmt, Vishva/UniFAQ_DataSET, Wikidepia/IndoParaCrawl, Wikidepia/IndoSQuAD, XiangXiang/clt, Yves/fhnw_swiss_parliament, abhishek/autonlp-data-imdb_eval, abwicke/C-B-R, abwicke/koplo, adamlin/re_dial, ajmbell/test-dataset, alireza655/alireza655, allenai/c4, ancs21/viwiki-18042021, anukaver/EstQA, aschvin/fhnw_test, ashish-shrivastava/dont-know-dataset, astarostap/antisemitic-tweets, astarostap/antisemitic_tweets, athivvat/thai-rap-lyrics, ausgequetschtem/jtrddfhfgh, bavard/personachat_truecased, bemanningssitua/dplremjfjfj, berkergurcay/2020-10K-Reports, bsc/ancora-ca-ner, bsc/sts-ca, bsc/tecla, bsc/viquiquad, bsc/xquad-ca, caca/zscczs, canwenxu/dogwhistle, ccccccc/hdjw_94ejrjr, cdminix/mgb1, cemigo/taylor_vs_shakes, cemigo/test-data, cheulyop/ksponspeech, clarin-pl/cst-wikinews, clarin-pl/nkjp-pos, clarin-pl/polemo2-official, classla/copa_hr, classla/hr500k, classla/reldi_hr, classla/reldi_sr, classla/setimes_sr, cnrcastroli/aaaa, congpt/dstc23_asr, dasago78/dasago78dataset, david-wb/zeshel, deepset/germandpr, deepset/germanquad, dfgvhxfgv/fghghj, dgknrsln/Yorumsepeti, dispenst/jhghdghfd, dispix/test-dataset, dynabench/dynasent, dynabench/qa, eason929/test, edfews/szdfcszdf, edsas/fgrdtgrdtdr, edsas/grttyi, ervis/aaa, ervis/qqq, formermagic/github_python_1m, formu/CVT, fulai/DuReader, fuliucansheng/data_for_test, german-nlp-group/german_common_crawl, godzillavskongonlinetv/ergfdg, godzillavskongonlinetv/godzillavskongfullmovie, gpt3mix/rt20, gpt3mix/sst2, gustavecortal/fr_covid_news, hartzeer/kdfjdshfje, hfface/poopi, huseinzol05/translated-The-Pile, iamshsdf/sssssssssss, jaimin/wav2vec2-large-xlsr-gujarati-demo, jdepoix/junit_test_completion, jglaser/binding_affinity, jimregan/clarinpl_sejmsenat, jimregan/clarinpl_studio, jmamou/augmented-glue-sst2, joelito/ler, joelito/sem_eval_2010_task_8, julien-c/dummy-dataset-from-colab, k-halid/ar, karinev/lanuitdudroit, katoensp/VR-OP, kmyoo/klue-tc-dev, lavis-nlp/german_legal_sentences, lhoestq/custom_squad, lhoestq/squad, lhoestq/test, lhoestq/wikipedia_bn, lkiouiou/o9ui7877687, lohanna/testedjkcxkf, lucien/sciencemission, lucien/voacantonesed, lucien/wsaderfffjjjhhh, lucio/common_voice_eval, majod/CleanNaturalQuestionsDataset, makanan/umich, medzaf/test, metalearning/kaggale-nlp-tutorial, mksaad/Arabic_news, mmm-da/rutracker_anime_torrent_titles, mohsenfayyaz/toxicity-classification-datasets, mulcyber/europarl-mono, mustafa12/db_ee, mustafa12/edaaaas, mustafa12/thors, naver-clova-conversation/klue-tc-dev-tsv, naver-clova-conversation/klue-tc-tsv, naver-clova-conversation-ul/klue-tc-dev, nbroad/few-nerd, nucklehead/ht-voice-dataset, oelkrise/CRT, osanseviero/llama_test, parivartanayurveda/Malesexproblemsayurvedictreatment, patrickvonplaten/librispeech_asr_dummy, patrickvonplaten/scientific_papers_dummy, pdesoyres/test, peixian/equity_evaluation_corpus, peixian/rtGender, persiannlp/parsinlu_entailment, persiannlp/parsinlu_query_paraphrasing, persiannlp/parsinlu_reading_comprehension, persiannlp/parsinlu_sentiment, persiannlp/parsinlu_translation_en_fa, persiannlp/parsinlu_translation_fa_en, piEsposito/br-quad-2.0, piEsposito/br_quad_20, piEsposito/squad_20_ptbr, princeton-nlp/datasets-for-simcse, priya3301/Graduation_admission, priya3301/tes, priya3301/test, projectaligned/reddit_writingprompts_full, rony/soccer-dialogues, roskoN/dstc8-reddit-corpus, salesken/Paraphrase_category_detection, sdfufygvjh/fgghuviugviu, seamew/Weibo, seamew/amazon_reviews_zh, seamew/weibo_avg, shahrukhx01/questions-vs-statements, sharejing/BiPaR, sileod/metaeval, sismetanin/rureviews, smallv0221/my-test, somaimanguyat/movie21, somaimanguyat/xiomay, spacemanidol/msmarco_passage_ranking, ssasaa/gghghgh, sshleifer/pseudo_bart_xsum, stas/openwebtext-10k, stas/wmt14-en-de-pre-processed, stas/wmt16-en-ro-pre-processed, stiel/skjdhjkasdhasjkd, subiksha/OwnDataset, susumu2357/squad_v2_sv, tals/test, thiemowa/argumentationreviewcorpus, thiemowa/empathyreviewcorpus, tommy19970714/common_voice, tommy19970714/jsut_asr, tommy19970714/jsut_asr_hiragana, tommy19970714/jsut_asr_hiragana_small, tommy19970714/laborotvspeech, turingbench/TuringBench, uasoyasser/rgfes, vasudevgupta/bigbird-tokenized-natural-questions, vasudevgupta/data, vasudevgupta/natural-questions-validation, vasudevgupta/temperature-distribution-2d-plate, vasudevgupta/temperature-distribution-3d-cylinder, vctc92/sdsd, vctc92/test, versae/adobo, vershasaxena91/datasets, vershasaxena91/squad_multitask, w11wo/imdb-javanese, webek18735/ddvoacantonesed, webek18735/dhikhscook, wmt/europarl, wmt/news-commentary, wmt/uncorpus, wmt/wikititles, wmt/wmt10, wmt/wmt13, wmt/wmt14, wmt/wmt15, wmt/wmt16, wmt/wmt17, wmt/wmt18, wmt/wmt19, yluisfern/PBU\n"
     ]
    }
   ],
   "source": [
    "datasets_list = list_datasets()\n",
    "print(', '.join(dataset for dataset in datasets_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb58e8c-562f-4fbe-bbed-860b85c2f429",
   "metadata": {},
   "source": [
    "Usaremos para efectos de NER el dataset ```conll2002``` con los tags para la tarea asociada. Si se desea utilizar un dataset distinto fuera de los de ejemplo, se recomienda revisar el siguiente [enlace](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ce43908-e862-40bd-bdbb-8c0137dd5570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78a3d8a8196409d9d1ed2e4cf38d513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2632.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ec5154eca54742a39710066d304efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2012.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading and preparing dataset conll2002/es (download: 3.95 MiB, generated: 8.87 MiB, post-processed: Unknown size, total: 12.82 MiB) to /Users/iacastro/.cache/huggingface/datasets/conll2002/es/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bdbec0b4cce4fdca8587bd69ae4bd0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=712623.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6dcf49b64f45f9a8037f41c1269018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=141104.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268f3c8bd3eb491b86b03ce756151598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=137919.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ba729576634378bd73b3026685ab59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76cfbcf65e3c4fd2b27ee8417534ddf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20947175ec3248c3afa444c80f7c884e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset conll2002 downloaded and prepared to /Users/iacastro/.cache/huggingface/datasets/conll2002/es/1.0.0/a3a8a8612caf57271f5b35c5ae1dd25f99ddb9efb9c1667abaa70ede33e863e5. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "   'conll2002', 'es')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c624e354-60e9-4943-b194-8cff1eadb1ab",
   "metadata": {},
   "source": [
    "El dataset es un diccionario que contiene separación para ```train```, ```validation``` y ```test```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26c2d465-82ad-4da3-86db-041d1410f49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'ner_tags'],\n",
       "        num_rows: 8324\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'ner_tags'],\n",
       "        num_rows: 1916\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'ner_tags'],\n",
       "        num_rows: 1518\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb0c4e-eb64-47ec-ac6e-93d3431b9faa",
   "metadata": {},
   "source": [
    "Para obtener la representación en string de las clases para los token de NER, se pueden extraer directamente del campo ```features```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0aa5d70-86a8-48ee-891f-2dd8379d3a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].features[f\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dce61b3-c6a8-4ee7-bed0-404bcd48a937",
   "metadata": {},
   "source": [
    "Estas entidades están en el formato BIO\n",
    "\n",
    "* B por \"beginning\", se refiera a tokens que están al comienzo de cada entidad\n",
    "* I por \"intermediate\", se refieren a tokens que están en medio de cada entidad\n",
    "* O por \"outside\", se refieren a tokens que no pertenecen a entidades relevantes para el problema en cuestión\n",
    "\n",
    "Por otro lado, para este dataset específico se tienen las siguientes entidades\n",
    "\n",
    "* PER para persona\n",
    "* ORG para organización\n",
    "* LOC para ubicación\n",
    "* MISC para misceláneo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b9fd10-0c85-40c1-ac91-ff104b012be5",
   "metadata": {},
   "source": [
    "De lo anterior podemos obtener sólo la lista con las entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ceb4abe2-a39b-4fe5-b72e-76617bd9b411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = dataset[\"train\"].features[f\"{task}_tags\"].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c92c90-c9ed-4c9f-999f-8bd1c7c3836d",
   "metadata": {},
   "source": [
    "Definamos ahora una función para formatear y cargar elementos aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efeb2908-d631-449e-9094-b1b4cfa22f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd21897f-e276-45c3-81db-3f2890571627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>542</td>\n",
       "      <td>[O, O, O, O, O, O, B-ORG, O, O, O, O, O, O, O, O, B-MISC, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[VMI, CS, DA, NC, SP, DA, NC, Fc, CS, DA, NC, SP, DA, NC, SP, NC, Fc, Fe, RN, VAI, VMP, SP, PI, Fe, CC, VMI, CS, SP, RG, Fc, DA, NC, SP, NC, VAI, VSP, DA, RG, AQ, SP, DA, NC, SP, DD, NC, CC, SP, DA, NC, Fc, CC, CS, VMM, VMI, RG, SP, DA, NC, SP, NC, Fp]</td>\n",
       "      <td>[Añadió, que, las, actuaciones, de, la, Administración, ,, como, la, rebaja, de, los, módulos, del, IRPF, ,, \", no, han, valido, para, nada, \", y, explicó, que, hasta, ahora, ,, los, productores, de, secano, han, sido, los, más, perjudicados, por, el, incremento, de, este, insumo, y, de, los, fertilizantes, ,, pero, si, continúa, perjudicará, también, a, las, explotaciones, de, regadío, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7914</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-PER, I-PER, O, O, O, O, B-PER, I-PER, O, O, O, O]</td>\n",
       "      <td>[SP, RG, NC, Fc, DA, NC, SP, DA, NC, Fc, DA, NC, AQ, AQ, Fc, CC, DA, NC, NC, AQ, VMI, NC, AQ, Fp]</td>\n",
       "      <td>[De, igual, forma, ,, el, líder, de, la, montaña, ,, el, holandés, Karsten, Kroon, ,, y, el, español, Angel, Vicioso, sufren, contusiones, varias, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7014</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[SP, DA, AO, DN, NC, DA, NC, SP, NC, VMI, SP, NC, AQ, NC, SP, NC, SP, VMN, DI, NC, SP, DA, NC, CC, DI, NC, SP, DA, NC, Fp]</td>\n",
       "      <td>[En, los, últimos, quince, días, los, bomberos, de, Vigo, llevaron, a, cabo, numerosas, acciones, de, protesta, para, reclamar, una, mejora, en, los, medios, y, un, aumento, en, la, plantilla, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6324</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-PER, O]</td>\n",
       "      <td>[Fe, VAI, NC, SP, DA, NC, SP, AQ, CC, NC, Fc, RG, DA, AQ, Fc, CC, CS, PI, VMI, NC, AQ, CC, VMI, DP, NC, PP, VMI, AQ, CS, SP, DA, RG, P0, PP, VMS, RG, AQ, Fe, Fc, VMI, NC, Fp]</td>\n",
       "      <td>[\", Hay, atrasos, en, los, pagos, a, militares, y, policías, ,, especialmente, los, militares, ,, y, si, uno, tiene, gente, armada, y, pretende, su, lealtad, me, parece, lógico, que, por, lo, menos, se, le, tenga, bien, atendida, \", ,, subrayó, Galaverna, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1533</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[PP, Fc, SP, VMM, Fc, SP, DA, NC, SP, DA, NC, AQ, Fpa, Z, SP, PN, Fpt, CC, SP, DA, AQ, Fpa, Z, SP, PN, Fpt, Fc, DA, PR, VMI, DA, NC, AQ, SP, DI, NC, AQ, AQ, CC, DA, NC, SP, NC, VMI, SP, NC, CC, SP, NC, Fp]</td>\n",
       "      <td>[Ello, ,, según, justifica, ,, por, el, alza, de, la, demanda, interna, (, +7,1, por, ciento, ), y, de, la, externa, (, +13,2, por, ciento, ), ,, lo, que, refleja, el, efecto, positivo, de, una, situación, internacional, favorable, y, la, depreciación, del, euro, respeto, del, dólar, y, del, yen, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2436</td>\n",
       "      <td>[O, O, O, O, B-MISC, I-MISC, O, O, O, B-MISC, I-MISC, O, O, O, B-MISC, O, O, O, B-MISC, I-MISC, O, O, O, B-MISC, I-MISC, I-MISC, I-MISC, I-MISC, O, O, B-MISC, I-MISC, I-MISC, I-MISC, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-MISC, I-MISC, I-MISC, I-MISC, I-MISC, I-MISC, O, O]</td>\n",
       "      <td>[SP, NC, CS, Fe, DP, NC, Fe, Fc, Fe, PP, VMI, Fe, Fc, Fe, NC, Fe, Fc, Fe, NC, AQ, Fe, Fc, Fe, PP, VMI, DI, NC, AQ, Fe, Fe, DA, NC, DA, NC, Fe, Fc, SP, RG, SP, DN, NC, Fc, DA, NC, SP, PP, NC, Fc, DA, NC, VMI, DI, NC, SP, DA, NC, PR, VMI, DA, NC, AQ, SP, DA, NC, SP, NC, Z, Fc, SP, DA, NC, Fe, AQ, NC, SP, DA, NC, AQ, Fe, Fp]</td>\n",
       "      <td>[Con, canciones, como, \", Mi, Jaca, \", ,, \", La, parrala, \", ,, \", Tatuaje, \", ,, \", Raska, yu, \", ,, \", Yo, vendo, unos, ojos, negros, \", \", La, luna., el, toro, \", ,, hasta, más, de, cuatrocientas, composiciones, ,, la, mayoría, de, ellas, coplas, ,, el, autor, hace, un, inventario, de, las, canciones, que, alimentaron, el, gusto, popular, durante, la, postguerra, hasta, llegar., 1975, ,, con, el, capítulo, \", Pasión., muerte, de, la, sentimentalidad, franquista, \", .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>511</td>\n",
       "      <td>[O, O, O, B-ORG, O, B-PER, I-PER, I-PER, O, O, O, O, O, O, B-PER, I-PER, I-PER, O, O, O, O, O, O, O, O, O, O, B-PER, I-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O]</td>\n",
       "      <td>[DA, NC, SP, NC, Fc, NC, AQ, NC, Fc, SP, PR, VMI, DA, NC, AQ, NC, AQ, CC, DA, NC, SP, NC, Fc, DA, NC, CC, NC, AQ, NC, Fc, SP, DI, NC, SP, NC, VMI, NC, SP, DD, NC, PR, VMI, DA, NC, AQ, SP, DD, NC, CC, AQ, AQ, SP, Z, NC, CS, SP, DA, NC, VMI, SP, NC, Fp]</td>\n",
       "      <td>[El, director, del, IVAM, ,, Juan, Manuel, Bonet, ,, al, que, acompañaba, el, comisario, Joan, Ramón, Escrivá, y, el, hijo, del, artista, ,, el, pintor, y, diseñador, Jordi, Ballester, ,, en, una, rueda, de, prensa, dio, detalles, de, esta, exposición, que, supone, la, recuperación, definitiva, de, este, escultor, y, dibujante, valenciano, de, 90, años, que, en, la, actualidad, reside, en, Barcelona, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7836</td>\n",
       "      <td>[O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-PER, O]</td>\n",
       "      <td>[SP, NC, Fc, Fe, DA, NC, AQ, VSI, DI, NC, AQ, SP, NC, CC, PD, NC, SP, DA, NC, SP, DA, NC, PR, VMI, DA, Fz, NC, NC, Fz, Fe, Fc, SP, VMI, Fp]</td>\n",
       "      <td>[En, Argentina, ,, \", el, gas, natural, es, una, fuente, principal, de, energía, y, eso, ayuda, en, la, reducción, de, los, gases, que, causan, el, ', efecto, invernadero, ', \", ,, según, Logan, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1764</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, I-LOC, O, B-LOC, O, B-LOC, O, B-LOC, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[VAI, NC, AQ, SP, DA, NC, SP, DA, NC, AQ, AQ, SP, NC, SP, NC, SP, NC, AQ, SP, DA, NC, Fc, AQ, Fc, NC, CC, AQ, Fc, CS, CS, SP, DA, NC, SP, DA, NC, VMI, RG, AQ, Fx, VAI, NC, AQ, SP, NC, CC, SP, DA, NC, SP, NC, CC, RG, AQ, SP, DA, NC, SP, DD, NC, Fp]</td>\n",
       "      <td>[Habrá, intervalos, nubosos, en, el, resto, de, la, mitad, norte, peninsular, con, posibilidad, de, chubascos, en, zonas, montañosas, de, La, Rioja, ,, Navarra, ,, Aragón, y, Cataluña, ,, mientras, que, en, el, resto, de, la, Península, estará, poco, nuboso, ;, habrá, intervalos, nubosos, en, Baleares, y, en, el, norte, de, Canarias, y, poco, nuboso, en, el, sur, de, esta, comunidad, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>532</td>\n",
       "      <td>[B-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, O, B-ORG, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[NC, VMI, DD, NC, NC, RG, SP, PR, DA, AQ, NC, SP, NC, CC, NC, SP, NC, Fpa, NC, CC, NC, Fpt, VMS, DI, AQ, NC, SP, NC, Fp]</td>\n",
       "      <td>[Lamela, hizo, estas, declaraciones, horas, después, de, que, los, principales, operadores, de, gasolina, y, gasóleo, del, país, (, Cepsa, y, Repsol, ), anunciaran, una, nueva, subida, de, precios, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e1ed4b-4993-48f3-91dd-5db7d4be4ced",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba9f4c2-9591-4293-8bde-c3fc716e2e3d",
   "metadata": {},
   "source": [
    "Primero tokenizamos el texto que le ingestaremos al modelo según el formato que éste espera para el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e629c04-b825-4b20-a7de-f644dcea666d",
   "metadata": {},
   "source": [
    "Para tener esto, instanciaremos el tokenizador con la función ```AutoTokenizer.from_pretrained```, la cual permite:\n",
    "\n",
    "* Obtener un tokenizador que corresponda a la arquitectura de modelo que usaremos\n",
    "* Descargar el vocabulario utilizado cuando se pre-entrenó este checkpoint de modelo en particular\n",
    "\n",
    "El vocabulario se guarda en caché, de manera de no descargarlo nuevamente cuando se ejecute la celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4008288c-97d8-408e-8b44-deba6f766386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fd85b3e3ad4326b0b538190c63a860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=247723.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63edae4114e04de1ade49c3c39410bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=486125.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49264540ee274383b5b37b7d8a7080b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=134.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6052b8f823254dc0a8ace378d856fc17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=310.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c73d8c-84ba-4543-ad6e-7800079e7474",
   "metadata": {},
   "source": [
    "Validamos si el modelo tiene un tokenizador rápido mantenido por librerías (es necesario en este notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ea37914-0f53-4212-a398-8e204acd62d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3d81cd-9641-4c18-b1b1-d27b586c41a6",
   "metadata": {},
   "source": [
    "A continuación, para probar su funcionamiento se tomará una sentencia de prueba que nos permitirá ver como llamar al tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbd5af31-4020-42c9-bc06-63b9a9c5834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"¡Hola, esta es una oración!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c954145-be33-4b4b-ab05-a65e60f010ca",
   "metadata": {},
   "source": [
    "Primero para un string completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1e54334-cf4c-42e6-bbe3-c9bc2b8ba6c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [4, 1120, 1734, 1019, 1149, 1028, 1091, 9301, 1109, 5], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4758e7d7-062e-4ac1-aca1-2ea177e814dc",
   "metadata": {},
   "source": [
    "O como en este caso, separado por palabras/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4581e7bd-b9d1-4429-9f9d-f101844ae475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [4, 1120, 1734, 1019, 1149, 1028, 1091, 9301, 1109, 5], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(test_sentence.split(), is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54df10d3-0044-4ca5-a9d3-8c0112fa06d2",
   "metadata": {},
   "source": [
    "Tomemos ahora un texto de ejemplo desde nuestro conjunto de entrenamiento con los tokens. La idea aquí es validar si el modelo posee tokenizado sub palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08059eb0-123d-46bb-9656-b0e26df2bc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Por', 'su', 'parte', ',', 'el', 'Abogado', 'General', 'de', 'Victoria', ',', 'Rob', 'Hulls', ',', 'indicó', 'que', 'no', 'hay', 'nadie', 'que', 'controle', 'que', 'las', 'informaciones', 'contenidas', 'en', 'CrimeNet', 'son', 'veraces', '.']\n",
      "[0, 0, 0, 0, 0, 1, 2, 0, 5, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "example = dataset[\"train\"][5]\n",
    "print(example[\"tokens\"])\n",
    "print(example[\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8456bf71-8c9f-45f5-92f2-242bda90882b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'por', 'su', 'parte', ',', 'el', 'abogado', 'general', 'de', 'victoria', ',', 'rob', 'hul', '##ls', ',', 'indicó', 'que', 'no', 'hay', 'nadie', 'que', 'control', '##e', 'que', 'las', 'informaciones', 'contenidas', 'en', 'crimen', '##et', 'son', 'vera', '##ces', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de4f5eb-45cb-4c68-b0e4-f953c5697e43",
   "metadata": {},
   "source": [
    "Se observa que la palabra \"hulls\" se dividió en 2 subtokens, también \"crimenet\" y \"veraces\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1d038-253b-4bdd-ab35-af5dbe904318",
   "metadata": {},
   "source": [
    "De lo anterior se ve que se requiere un preprocesamiento para manejar esos subtokens, además de caracteres especiales como ```[CLS]``` o ```[SEP]```. Podemos ver esto si comparamos los tags originales contra los tokenizados en el largo de cada uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c0d3010-fcd8-475e-8197-2138a49c2be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 35)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example[f\"{task}_tags\"]), len(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488977d9-4df8-415e-978a-b61f2d9cd5b5",
   "metadata": {},
   "source": [
    "Vemos que se generaron algunos tokens adicionales. Para recuperarlos, podemos usar los ID de tokenizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "daefb9d8-0102-42e6-98df-c8bbb47938a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 12, 13, 14, 15, 16, 17, 18, 19, 19, 20, 21, 22, 23, 24, 25, 25, 26, 27, 27, 28, None]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_input.word_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d832fc45-fdc7-424b-a309-3629992826d9",
   "metadata": {},
   "source": [
    "Acá vemos que los caracteres especiales se identifican como ```None```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "142cedc6-0055-44f7-8e68-15790d0b66d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 35\n"
     ]
    }
   ],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[f\"{task}_tags\"][i] for i in word_ids]\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7449f7c5-943b-4d85-a820-27a3c4b12b4a",
   "metadata": {},
   "source": [
    "Aquí se colocan los token especiales como -100 (índice ignorado por PyTorch) y las etiquetas de todos los demás tokens por la etiqueta de la palabra sobre la cual se originan. Otra estrategia es colocar la etiqueta al primer token de una palabra, y luego -100 a todos los subtokens obtenidos de la misma. En el código a continuación se ponen ambos enfoques, separados por el flag ```label_all_tokens``` (```True``` para primer enfoque y ```False``` para el segundo respectivamente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74e3199b-bd87-478c-85c9-2289bd5e4419",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1b54f09-2ce2-47c7-a32f-f3e4ed489251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e59bc9-86be-4b25-b01f-201f2bcffcd7",
   "metadata": {},
   "source": [
    "La función permite procesar múltiples oraciones a la vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "995aa0cf-1fb3-47eb-b009-e704cd702184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[4, 26890, 1147, 6154, 1135, 1019, 2581, 1533, 1147, 1818, 30955, 1135, 1008, 5], [4, 1139, 5], [4, 1039, 4682, 1661, 1081, 1520, 1019, 30374, 1005, 12103, 1019, 17774, 2066, 1032, 3503, 1009, 2736, 2235, 1097, 4190, 1074, 2074, 5957, 20169, 2767, 1012, 1091, 4448, 1009, 4730, 1041, 4153, 9570, 1503, 1039, 5333, 1009, 1067, 3551, 7412, 1009, 1032, 2030, 1008, 5], [4, 1032, 7078, 1081, 4682, 1661, 1394, 1646, 1649, 1009, 1041, 1044, 5804, 1081, 3151, 9396, 1081, 1520, 1009, 4848, 1147, 6154, 1135, 1057, 15969, 17892, 1012, 27032, 1044, 8573, 3557, 1040, 16203, 1039, 2621, 2306, 1039, 11717, 1009, 1032, 4019, 1009, 1041, 1085, 1845, 1041, 1086, 5060, 1494, 7371, 1887, 10816, 1926, 1246, 1039, 9029, 1012, 2589, 1009, 1032, 4448, 5728, 3283, 1008, 5], [4, 1149, 4448, 1005, 3686, 3664, 1044, 1935, 1009, 5386, 1019, 1526, 1035, 1039, 1041, 1075, 1764, 5016, 1167, 1035, 1186, 1009, 1044, 7098, 1009, 5996, 1019, 1040, 15411, 1926, 1246, 5015, 1009, 8490, 1040, 10938, 1319, 13201, 1166, 1068, 2152, 27959, 1019, 2783, 1041, 10602, 1009, 4276, 1009, 8323, 1040, 6350, 10199, 1008, 5]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 5, 0, 5, 0, 0, 0, 0, 0, 3, 3, 0, 0, -100], [-100, 0, -100], [-100, 0, 1, 2, 2, 2, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, -100], [-100, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 0, -100], [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_align_labels(dataset['train'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4981e444-a0a4-4285-9cb8-339d6a598af5",
   "metadata": {},
   "source": [
    "Para aplicar la función a todo el dataset, usamos ```map``` para aplicar sobre ```train```, ```validation``` y ```test``` en una sola operación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "847660b6-f969-4880-ba90-41855a3bd77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3d4f31b2d448dfaeba6b635b23b925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a46c5356834b3882f26b3c5ef8d22d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20fb97f534494f48bc63aa7ab6bd3845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f133b-5807-46c9-b305-c13e28e8d041",
   "metadata": {},
   "source": [
    "## Ajuste fino de modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a9d00-b6d8-4657-af65-5535ad27b081",
   "metadata": {},
   "source": [
    "Con los datos listos, descargamos el modelo pre-entrenado y lo ajustamos. Dado que todas las tareas a realizar son para clasificación de tokens, usamos la clase ```AutoModelForTokenClassification```. Como con el tokenizado, usamos la función ```from_pretrained``` para descargar y cachear el modelo. Lo único que debemos especificar para el problema es la cantidad de etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "534568eb-b533-437e-bfea-cffbb1f10577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bd1eb2-d2c5-4f75-9d14-b6f54b498489",
   "metadata": {},
   "source": [
    "La advertencia dice que perdemos algunos pesos asociados a las capas de vocabulario y otras inicializadas aleatoriamente (capas del clasificador). Lo anterior es esperado, ya que estamos removiento las capas finales para ajustarlas con los datos que tenemos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faac28e8-fa70-46fe-b60e-d56c8bb5fa7d",
   "metadata": {},
   "source": [
    "Para instanciar un ```Trainer```, necesitamos definir 3 cosas más. La más importante son los ```TrainingArgumnts```, la cual es una clase que contiene todos los atributos para personalizar el entrenamiento. Ésta requiere un nombre de carpeta, el cual se usará para guardar los checkpoints del modelo y los otros argumentos son opcionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e34e17b-2d5c-4ede-adb2-ec1dc890d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"test-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ced6c5-81b2-439b-bb6e-86d42f067499",
   "metadata": {},
   "source": [
    "Aquí vemos que la evaluación se hace en cada epoch, se manipula la tasa de aprendizaje, se usa el tamaño del batch y se define el número de epochs para el entrenamiento, así como el decaimiento de los pesos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac156b84-8d0d-4d44-b8c4-a015969fa565",
   "metadata": {},
   "source": [
    "Luego necesitaremos un ```data_collator``` para que tomen los ejemplos por lotes mientras le aplica padding del mismo tamaño (el padding será del largo del tamaño del más largo ejemplo). Existe un collator para esta tarea en la librería de Transformers, que no sólo aplica el padding sobre las entradas, pero también en las etiquetas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e896c613-e639-44e1-8fbd-ea3c10d8d28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8baab93-a4a1-4378-8250-7f10b12a10c8",
   "metadata": {},
   "source": [
    "Finalmente definimos la métrica sobre la cual se evaluará el entrenamiento, en este caso representada por el framework ```seqeval``` [documentación asociada](https://github.com/chakki-works/seqeval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0e18c1e-890d-4e36-8208-642834227b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4793c2ffd34c4047b9a9e8173f0017c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2482.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9149b-bc16-44c0-ad72-21ff834c1823",
   "metadata": {},
   "source": [
    "La métrica toma una lista de etiquetas para las predicciones y referencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be5f1f64-e47c-451e-91be-534ce45c4105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'MISC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'PER': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label_list[i] for i in example[f\"{task}_tags\"]]\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c854e-d21c-4d72-a233-2633991d033a",
   "metadata": {},
   "source": [
    "Necesitaremos post procesar las predicciones\n",
    "\n",
    "* Seleccionar el índice predicho (salida con máximo logit) para cada token\n",
    "* Convertir el token a su etiqueta en string asociada\n",
    "* Ignorar todo lo que tenga la etiqueta -100\n",
    "\n",
    "La siguiente función realiza todo este post procesamiento sobre el resultado ```Trainer.evaluate``` antes de aplicar la métrica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74533418-c39e-435c-b345-54d024e52995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5821b5-a9f1-4a49-8454-0a45e84b1944",
   "metadata": {},
   "source": [
    "Notar que se botan las métricas por etiqueta (la agregaremos luego) y se focalizan sobre los globales. Con lo anterior está listo para entregarlo al ```Trainer```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17833e7b-0c88-4602-b52b-b5b6c33ef028",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b12c73ec-e32f-4fbc-8fe0-37b9254bb92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 8:00:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.210600</td>\n",
       "      <td>0.146080</td>\n",
       "      <td>0.796673</td>\n",
       "      <td>0.774286</td>\n",
       "      <td>0.785320</td>\n",
       "      <td>0.957432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>0.137706</td>\n",
       "      <td>0.812020</td>\n",
       "      <td>0.830790</td>\n",
       "      <td>0.821298</td>\n",
       "      <td>0.962588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.060100</td>\n",
       "      <td>0.143024</td>\n",
       "      <td>0.822331</td>\n",
       "      <td>0.835625</td>\n",
       "      <td>0.828925</td>\n",
       "      <td>0.964022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1563, training_loss=0.11845848412370347, metrics={'train_runtime': 28845.8315, 'train_samples_per_second': 0.054, 'total_flos': 51706117436472.0, 'epoch': 3.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a780d882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 20:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.1430237740278244,\n",
       " 'eval_precision': 0.8223312518584597,\n",
       " 'eval_recall': 0.8356247167245807,\n",
       " 'eval_f1': 0.8289246908954664,\n",
       " 'eval_accuracy': 0.9640221699481197,\n",
       " 'eval_runtime': 621.9279,\n",
       " 'eval_samples_per_second': 3.081,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "272392f9-1c62-4f92-8fad-0de2cb5d8843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC': {'precision': 0.8322932917316692,\n",
       "  'recall': 0.8508771929824561,\n",
       "  'f1': 0.8414826498422713,\n",
       "  'number': 1254},\n",
       " 'MISC': {'precision': 0.5792682926829268,\n",
       "  'recall': 0.521978021978022,\n",
       "  'f1': 0.5491329479768786,\n",
       "  'number': 728},\n",
       " 'ORG': {'precision': 0.8139450867052023,\n",
       "  'recall': 0.8553530751708428,\n",
       "  'f1': 0.8341355053683821,\n",
       "  'number': 2634},\n",
       " 'PER': {'precision': 0.9064356435643565,\n",
       "  'recall': 0.9141288067898152,\n",
       "  'f1': 0.9102659706686551,\n",
       "  'number': 2003},\n",
       " 'overall_precision': 0.8223312518584597,\n",
       " 'overall_recall': 0.8356247167245807,\n",
       " 'overall_f1': 0.8289246908954664,\n",
       " 'overall_accuracy': 0.9640221699481197}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd51e7-1edb-4ec5-8f51-dcee99c3d4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
