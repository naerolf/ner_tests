{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80225757-b0f7-4f3f-8de9-1df3c9610a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements_custom.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements_custom.txt\n",
    "transformers==4.6.1\n",
    "torch==1.8.1\n",
    "torchvision==0.9.1\n",
    "pandas==1.1.5\n",
    "sklearn==0.0\n",
    "matplotlib==3.4.2\n",
    "ipywidgets==7.6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b28edb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.6.1 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from -r requirements_custom.txt (line 1)) (4.6.1)\n",
      "Requirement already satisfied: torch==1.8.1 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from -r requirements_custom.txt (line 2)) (1.8.1)\n",
      "Requirement already satisfied: torchvision==0.9.1 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from -r requirements_custom.txt (line 3)) (0.9.1)\n",
      "Requirement already satisfied: pandas==1.1.5 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from -r requirements_custom.txt (line 4)) (1.1.5)\n",
      "Requirement already satisfied: sklearn==0.0 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from -r requirements_custom.txt (line 5)) (0.0)\n",
      "Requirement already satisfied: matplotlib==3.4.2 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from -r requirements_custom.txt (line 6)) (3.4.2)\n",
      "Requirement already satisfied: ipywidgets==7.6.3 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from -r requirements_custom.txt (line 7)) (7.6.3)\n",
      "Requirement already satisfied: requests in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (1.20.3)\n",
      "Requirement already satisfied: huggingface-hub==0.0.8 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (0.0.8)\n",
      "Requirement already satisfied: filelock in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (4.60.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (0.10.2)\n",
      "Requirement already satisfied: packaging in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (20.9)\n",
      "Requirement already satisfied: sacremoses in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (0.0.45)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from transformers==4.6.1->-r requirements_custom.txt (line 1)) (2021.4.4)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from torch==1.8.1->-r requirements_custom.txt (line 2)) (3.10.0.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from torchvision==0.9.1->-r requirements_custom.txt (line 3)) (8.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from pandas==1.1.5->-r requirements_custom.txt (line 4)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from pandas==1.1.5->-r requirements_custom.txt (line 4)) (2021.1)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from sklearn==0.0->-r requirements_custom.txt (line 5)) (0.24.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from matplotlib==3.4.2->-r requirements_custom.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from matplotlib==3.4.2->-r requirements_custom.txt (line 6)) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from matplotlib==3.4.2->-r requirements_custom.txt (line 6)) (0.10.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.0.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (7.22.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (5.3.4)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (3.5.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (5.1.3)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (5.0.5)\n",
      "Requirement already satisfied: six in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from cycler>=0.10->matplotlib==3.4.2->-r requirements_custom.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: jupyter-client in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (6.1.12)\n",
      "Requirement already satisfied: tornado>=4.2 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (6.1)\n",
      "Requirement already satisfied: pickleshare in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (3.0.17)\n",
      "Requirement already satisfied: pygments in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (2.9.0)\n",
      "Requirement already satisfied: decorator in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (5.0.9)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.17.0)\n",
      "Requirement already satisfied: backcall in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (52.0.0.post20210125)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.4.4)\n",
      "Requirement already satisfied: parso>=0.7.0 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.8.2)\n",
      "Requirement already satisfied: jupyter-core in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (4.7.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (3.2.0)\n",
      "Requirement already satisfied: ipython-genutils in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.17.3)\n",
      "Requirement already satisfied: wcwidth in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.2.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (6.4.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.9.4)\n",
      "Requirement already satisfied: prometheus-client in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.10.1)\n",
      "Requirement already satisfied: argon2-cffi in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (20.1.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.5.0)\n",
      "Requirement already satisfied: nbconvert in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (6.0.7)\n",
      "Requirement already satisfied: pyzmq>=17 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (20.0.0)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (3.0.0)\n",
      "Requirement already satisfied: pywin32>=1.0 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from jupyter-core->nbformat>=4.2.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (227)\n",
      "Requirement already satisfied: pywinpty>=0.5 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.5.7)\n",
      "Requirement already satisfied: cffi>=1.0.0 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.14.5)\n",
      "Requirement already satisfied: pycparser in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0.0rc2 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (2.0.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.1.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.8.4)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.5.3)\n",
      "Requirement already satisfied: bleach in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (3.3.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.4.3)\n",
      "Requirement already satisfied: defusedxml in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.7.1)\n",
      "Requirement already satisfied: testpath in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.4.4)\n",
      "Requirement already satisfied: async-generator in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.10)\n",
      "Requirement already satisfied: nest-asyncio in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (1.5.1)\n",
      "Requirement already satisfied: webencodings in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.6.3->-r requirements_custom.txt (line 7)) (0.5.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from requests->transformers==4.6.1->-r requirements_custom.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from requests->transformers==4.6.1->-r requirements_custom.txt (line 1)) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from requests->transformers==4.6.1->-r requirements_custom.txt (line 1)) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from requests->transformers==4.6.1->-r requirements_custom.txt (line 1)) (4.0.0)\n",
      "Requirement already satisfied: joblib in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from sacremoses->transformers==4.6.1->-r requirements_custom.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: click in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from sacremoses->transformers==4.6.1->-r requirements_custom.txt (line 1)) (8.0.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from scikit-learn->sklearn==0.0->-r requirements_custom.txt (line 5)) (1.6.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages (from scikit-learn->sklearn==0.0->-r requirements_custom.txt (line 5)) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements_custom.txt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a751f925-673d-49a2-9c4f-f7662855d738",
   "metadata": {},
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11796a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9afc53cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    }
   ],
   "source": [
    "!wget -N http://noisy-text.github.io/2017/files/wnut17train.conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd478562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wnut(file_path):\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    raw_text = file_path.read_text().strip()\n",
    "    raw_docs = re.split(r'\\n\\t?\\n', raw_text)\n",
    "    token_docs = []\n",
    "    tag_docs = []\n",
    "    for doc in raw_docs:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for line in doc.split('\\n'):\n",
    "            token, tag = line.split('\\t')\n",
    "            tokens.append(token)\n",
    "            tags.append(tag)\n",
    "        token_docs.append(tokens)\n",
    "        tag_docs.append(tags)\n",
    "\n",
    "    return token_docs, tag_docs\n",
    "\n",
    "texts, tags = read_wnut('wnut17train.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8706255d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['for', 'two', 'weeks', '.', 'Empire', 'State', 'Building']\n",
      "['O', 'O', 'O', 'O', 'B-location', 'I-location', 'I-location']\n"
     ]
    }
   ],
   "source": [
    "print(texts[0][10:17], tags[0][10:17], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "670481a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_tags, val_tags = train_test_split(texts, tags, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6f1dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = set(tag for doc in tags for tag in doc)\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eff62a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-creative-work': 0,\n",
       " 'O': 1,\n",
       " 'B-location': 2,\n",
       " 'I-creative-work': 3,\n",
       " 'B-person': 4,\n",
       " 'I-product': 5,\n",
       " 'B-corporation': 6,\n",
       " 'I-group': 7,\n",
       " 'I-person': 8,\n",
       " 'I-corporation': 9,\n",
       " 'B-product': 10,\n",
       " 'I-location': 11,\n",
       " 'B-group': 12}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb66557d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-creative-work',\n",
       " 1: 'O',\n",
       " 2: 'B-location',\n",
       " 3: 'I-creative-work',\n",
       " 4: 'B-person',\n",
       " 5: 'I-product',\n",
       " 6: 'B-corporation',\n",
       " 7: 'I-group',\n",
       " 8: 'I-person',\n",
       " 9: 'I-corporation',\n",
       " 10: 'B-product',\n",
       " 11: 'I-location',\n",
       " 12: 'B-group'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0aab9971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-corporation',\n",
       " 'B-creative-work',\n",
       " 'B-group',\n",
       " 'B-location',\n",
       " 'B-person',\n",
       " 'B-product',\n",
       " 'I-corporation',\n",
       " 'I-creative-work',\n",
       " 'I-group',\n",
       " 'I-location',\n",
       " 'I-person',\n",
       " 'I-product',\n",
       " 'O'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80117a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8911f4064e054140bf3e3e5a20ebfa84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6370a9de6fd14c01a851d894aabb4403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c11581c4db473389811b98cf9fcfd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n",
    "train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d3e0477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tags(tags, encodings):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "        # create an empty array of -100\n",
    "        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100\n",
    "        arr_offset = np.array(doc_offset)\n",
    "\n",
    "        # set labels whose first offset position is 0 and the second is not 0\n",
    "        doc_enc_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = doc_labels\n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels\n",
    "\n",
    "train_labels = encode_tags(train_tags, train_encodings)\n",
    "val_labels = encode_tags(val_tags, val_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c38b7993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class WNUTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
    "val_encodings.pop(\"offset_mapping\")\n",
    "train_dataset = WNUTDataset(train_encodings, train_labels)\n",
    "val_dataset = WNUTDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0637b42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983823cfcbd84655ad508f2583c9a9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0bdd7ccbf1742ffa98a71878551edc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/263M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForTokenClassification\n",
    "model = DistilBertForTokenClassification.from_pretrained('distilbert-base-cased', num_labels=len(unique_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd7cc3a-b80b-4b75-a9f3-74bab9f77eb5",
   "metadata": {},
   "source": [
    "# NER BETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73cff9bb-c77d-4420-af57-ceb1afbaf0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0da82b77-226a-49f0-b3fc-d63e5d5be088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mBertTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mvocab_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdo_lower_case\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdo_basic_tokenize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnever_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0munk_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'[UNK]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msep_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'[SEP]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpad_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'[PAD]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcls_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'[CLS]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmask_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'[MASK]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtokenize_chinese_chars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mstrip_accents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Construct a BERT tokenizer. Based on WordPiece.\n",
       "\n",
       "This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` which contains most of the main methods.\n",
       "Users should refer to this superclass for more information regarding those methods.\n",
       "\n",
       "Args:\n",
       "    vocab_file (:obj:`str`):\n",
       "        File containing the vocabulary.\n",
       "    do_lower_case (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
       "        Whether or not to lowercase the input when tokenizing.\n",
       "    do_basic_tokenize (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
       "        Whether or not to do basic tokenization before WordPiece.\n",
       "    never_split (:obj:`Iterable`, `optional`):\n",
       "        Collection of tokens which will never be split during tokenization. Only has an effect when\n",
       "        :obj:`do_basic_tokenize=True`\n",
       "    unk_token (:obj:`str`, `optional`, defaults to :obj:`\"[UNK]\"`):\n",
       "        The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
       "        token instead.\n",
       "    sep_token (:obj:`str`, `optional`, defaults to :obj:`\"[SEP]\"`):\n",
       "        The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n",
       "        sequence classification or for a text and a question for question answering. It is also used as the last\n",
       "        token of a sequence built with special tokens.\n",
       "    pad_token (:obj:`str`, `optional`, defaults to :obj:`\"[PAD]\"`):\n",
       "        The token used for padding, for example when batching sequences of different lengths.\n",
       "    cls_token (:obj:`str`, `optional`, defaults to :obj:`\"[CLS]\"`):\n",
       "        The classifier token which is used when doing sequence classification (classification of the whole sequence\n",
       "        instead of per-token classification). It is the first token of the sequence when built with special tokens.\n",
       "    mask_token (:obj:`str`, `optional`, defaults to :obj:`\"[MASK]\"`):\n",
       "        The token used for masking values. This is the token used when training this model with masked language\n",
       "        modeling. This is the token which the model will try to predict.\n",
       "    tokenize_chinese_chars (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
       "        Whether or not to tokenize Chinese characters.\n",
       "\n",
       "        This should likely be deactivated for Japanese (see this `issue\n",
       "        <https://github.com/huggingface/transformers/issues/328>`__).\n",
       "    strip_accents: (:obj:`bool`, `optional`):\n",
       "        Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n",
       "        value for :obj:`lowercase` (as in the original BERT).\n",
       "\u001b[1;31mFile:\u001b[0m           d:\\anaconda3\\envs\\ner_tests\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     LayoutLMTokenizer, DistilBertTokenizer\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BertTokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15841bf-f77e-476c-b742-47fb7340a85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer and the model\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"pytorch/\", do_lower_case=False)\n",
    "model = BertForMaskedLM.from_pretrained(\"pytorch/\")\n",
    "e = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9aded116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67bdfa63b9d4d22b0d2f0ac2b5d561c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56eb8dfe6fb244a589c30fc4eae57c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (16) to match target batch_size (1440).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ddaead50ab97>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'attention_mask'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\ner_tests\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\ner_tests\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"single_label_classification\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"multi_label_classification\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\ner_tests\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\ner_tests\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m   1048\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\ner_tests\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2691\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2693\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\ner_tests\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2383\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2384\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   2385\u001b[0m             \u001b[1;34m\"Expected input batch_size ({}) to match target batch_size ({}).\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2386\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (16) to match target batch_size (1440)."
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertForSequenceClassification, AdamW\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a780d882",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
